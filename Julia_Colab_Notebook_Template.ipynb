{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Breast Cancer Diagnostic using XGBoost in Julia***"
      ],
      "metadata": {
        "id": "9wAR86SorGEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Breast Cancer Wisconsin (Diagnostic) Data Set***\n",
        "\n",
        "**Predict wheter the cancer is benign or malignant**\n",
        "\n",
        "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
        "n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
        "\n",
        "This database is also available through the UW CS ftp server:\n",
        "ftp ftp.cs.wisc.edu\n",
        "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
        "\n",
        "Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "1) ID number\n",
        "2) Diagnosis (M = malignant, B = benign)\n",
        "3-32)\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus:\n",
        "\n",
        "a) radius (mean of distances from center to points on the perimeter)\n",
        "b) texture (standard deviation of gray-scale values)\n",
        "c) perimeter\n",
        "d) area\n",
        "e) smoothness (local variation in radius lengths)\n",
        "f) compactness (perimeter^2 / area - 1.0)\n",
        "g) concavity (severity of concave portions of the contour)\n",
        "h) concave points (number of concave portions of the contour)\n",
        "i) symmetry\n",
        "j) fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three\n",
        "largest values) of these features were computed for each image,\n",
        "resulting in 30 features. For instance, field 3 is Mean Radius, field\n",
        "13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "All feature values are recoded with four significant digits.\n",
        "\n",
        "Missing attribute values: none\n",
        "\n",
        "Class distribution: 357 benign, 212 malignant\n",
        "\n"
      ],
      "metadata": {
        "id": "o-rq0WxFsFs1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1r1bbb0yBv"
      },
      "source": [
        "# <img src=\"https://github.com/JuliaLang/julia-logo-graphics/raw/master/images/julia-logo-color.png\" height=\"100\" /> _Colab Notebook Template_\n",
        "\n",
        "## Instructions\n",
        "1. Work on a copy of this notebook: _File_ > _Save a copy in Drive_ (you will need a Google account). Alternatively, you can download the notebook using _File_ > _Download .ipynb_, then upload it to [Colab](https://colab.research.google.com/).\n",
        "2. If you need a GPU: _Runtime_ > _Change runtime type_ > _Harware accelerator_ = _GPU_.\n",
        "3. Execute the following cell (click on it and press Ctrl+Enter) to install Julia, IJulia and other packages (if needed, update `JULIA_VERSION` and the other parameters). This takes a couple of minutes.\n",
        "4. Reload this page (press Ctrl+R, or ⌘+R, or the F5 key) and continue to the next section.\n",
        "\n",
        "_Notes_:\n",
        "* If your Colab Runtime gets reset (e.g., due to inactivity), repeat steps 2, 3 and 4.\n",
        "* After installation, if you want to change the Julia version or activate/deactivate the GPU, you will need to reset the Runtime: _Runtime_ > _Factory reset runtime_ and repeat steps 3 and 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIeFXS0F0zww",
        "outputId": "21b74c00-48bd-4985-f48b-6739edf08923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Julia 1.7.2 on the current Colab Runtime...\n",
            "2022-05-17 18:12:42 URL:https://storage.googleapis.com/julialang2/bin/linux/x64/1.7/julia-1.7.2-linux-x86_64.tar.gz [123295596/123295596] -> \"/tmp/julia.tar.gz\" [1]\n",
            "Installing Julia package IJulia...\n",
            "Installing Julia package BenchmarkTools...\n",
            "Installing Julia package Plots...\n",
            "Installing Julia package CUDA...\n",
            "Installing IJulia kernel...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling julia kernelspec in /root/.local/share/jupyter/kernels/julia-1.7\n",
            "\n",
            "Successfully installed julia version 1.7.2!\n",
            "Please reload this page (press Ctrl+R, ⌘+R, or the F5 key) then\n",
            "jump to the 'Checking the Installation' section.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.7.2\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"IJulia BenchmarkTools Plots\"\n",
        "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -n \"$COLAB_GPU\" ] && [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  if [ \"$COLAB_GPU\" = \"1\" ]; then\n",
        "      JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
        "  fi\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo ''\n",
        "  echo \"Successfully installed `julia -v`!\"\n",
        "  echo \"Please reload this page (press Ctrl+R, ⌘+R, or the F5 key) then\"\n",
        "  echo \"jump to the 'Checking the Installation' section.\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OS3Ac017T1i"
      },
      "source": [
        "# Checking the Installation\n",
        "The `versioninfo()` function should print your Julia version and some other info about the system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EEzvvzCl1i0F",
        "outputId": "c5cf3689-1a19-4e5a-c092-c1fb726e4f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Julia Version 1.7.2\n",
            "Commit bf53498635 (2022-02-06 15:21 UTC)\n",
            "Platform Info:\n",
            "  OS: Linux (x86_64-pc-linux-gnu)\n",
            "  CPU: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "  WORD_SIZE: 64\n",
            "  LIBM: libopenlibm\n",
            "  LLVM: libLLVM-12.0.1 (ORCJIT, broadwell)\n",
            "Environment:\n",
            "  JULIA_NUM_THREADS = 2\n"
          ]
        }
      ],
      "source": [
        "versioninfo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YjM_qq54lCcs",
        "outputId": "d41d0b50-0aeb-4857-dda5-b20bda4e007e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  432.777 ms (2 allocations: 32.00 MiB)\n"
          ]
        }
      ],
      "source": [
        "using BenchmarkTools\n",
        "\n",
        "M = rand(2^11, 2^11)\n",
        "\n",
        "@btime $M * $M;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XciCcMAJOT3_",
        "outputId": "2de00f05-6b82-4f73-f58e-350b2c9844ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 17 18:29:36 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "  426.933 ms (2 allocations: 32.00 MiB)\n"
          ]
        }
      ],
      "source": [
        "if ENV[\"COLAB_GPU\"] == \"1\"\n",
        "    using CUDA\n",
        "\n",
        "    run(`nvidia-smi`)\n",
        "\n",
        "    # Create a new random matrix directly on the GPU:\n",
        "    M_on_gpu = CUDA.CURAND.rand(2^11, 2^11)\n",
        "    @btime $M_on_gpu * $M_on_gpu; nothing\n",
        "else\n",
        "    println(\"No GPU found.\")\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up Julia in Google colab**"
      ],
      "metadata": {
        "id": "djW7QUfzqYzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "]  add \"https://github.com/dmlc/XGBoost.jl.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6F2dSxcSdxb",
        "outputId": "208ffb18-7c25-4f13-f988-59a78cf7106a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m     Cloning\u001b[22m\u001b[39m git-repo `https://github.com/dmlc/XGBoost.jl.git`\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/dmlc/XGBoost.jl.git`\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m XGBoost_jll ─ v1.5.2+0\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Project.toml`\n",
            " \u001b[90m [009559a3] \u001b[39m\u001b[92m+ XGBoost v1.5.2 `https://github.com/dmlc/XGBoost.jl.git#master`\u001b[39m\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Manifest.toml`\n",
            " \u001b[90m [009559a3] \u001b[39m\u001b[92m+ XGBoost v1.5.2 `https://github.com/dmlc/XGBoost.jl.git#master`\u001b[39m\n",
            " \u001b[90m [a5c6f535] \u001b[39m\u001b[92m+ XGBoost_jll v1.5.2+0\u001b[39m\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mXGBoost_jll\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39mXGBoost\n",
            "  2 dependencies successfully precompiled in 2 seconds (150 already precompiled, 3 skipped during auto due to previous errors)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up XGBoost**"
      ],
      "metadata": {
        "id": "oRIosLGnsyWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "] build XGBoost"
      ],
      "metadata": {
        "id": "xrz-kDU3Tpnz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "using XGBoost"
      ],
      "metadata": {
        "id": "HZESI41_TzMp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Pkg; Pkg.add(\"DataFrames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akZpdINnUC2v",
        "outputId": "67fe87e4-30aa-447d-9d6d-748526a32451"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Crayons ───────── v4.1.1\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m InvertedIndices ─ v1.1.0\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PooledArrays ──── v1.4.2\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DataFrames ────── v1.3.4\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PrettyTables ──── v1.3.1\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Project.toml`\n",
            " \u001b[90m [a93c6f00] \u001b[39m\u001b[92m+ DataFrames v1.3.4\u001b[39m\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Manifest.toml`\n",
            " \u001b[90m [a8cc5b0e] \u001b[39m\u001b[92m+ Crayons v4.1.1\u001b[39m\n",
            " \u001b[90m [a93c6f00] \u001b[39m\u001b[92m+ DataFrames v1.3.4\u001b[39m\n",
            " \u001b[90m [41ab1584] \u001b[39m\u001b[92m+ InvertedIndices v1.1.0\u001b[39m\n",
            " \u001b[90m [2dfb63ee] \u001b[39m\u001b[92m+ PooledArrays v1.4.2\u001b[39m\n",
            " \u001b[90m [08abe8d2] \u001b[39m\u001b[92m+ PrettyTables v1.3.1\u001b[39m\n",
            " \u001b[90m [9fa8497b] \u001b[39m\u001b[92m+ Future\u001b[39m\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mInvertedIndices\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mPooledArrays\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mCrayons\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mPrettyTables\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39mDataFrames\n",
            "  5 dependencies successfully precompiled in 25 seconds (152 already precompiled, 3 skipped during auto due to previous errors)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using DataFrames"
      ],
      "metadata": {
        "id": "nC1uFv26Uwc1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Pkg; Pkg.add(\"CSV\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e985e59-7748-4914-b279-8c4e1317aea9",
        "id": "Ro83qy7zU90M"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CodecZlib ────────── v0.7.0\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SentinelArrays ───── v1.3.12\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m WeakRefStrings ───── v1.4.2\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m InlineStrings ────── v1.1.2\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FilePathsBase ────── v0.9.18\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m TranscodingStreams ─ v0.9.6\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CSV ──────────────── v0.10.4\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Project.toml`\n",
            " \u001b[90m [336ed68f] \u001b[39m\u001b[92m+ CSV v0.10.4\u001b[39m\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Manifest.toml`\n",
            " \u001b[90m [336ed68f] \u001b[39m\u001b[92m+ CSV v0.10.4\u001b[39m\n",
            " \u001b[90m [944b1d66] \u001b[39m\u001b[92m+ CodecZlib v0.7.0\u001b[39m\n",
            " \u001b[90m [48062228] \u001b[39m\u001b[92m+ FilePathsBase v0.9.18\u001b[39m\n",
            " \u001b[90m [842dd82b] \u001b[39m\u001b[92m+ InlineStrings v1.1.2\u001b[39m\n",
            " \u001b[90m [91c51154] \u001b[39m\u001b[92m+ SentinelArrays v1.3.12\u001b[39m\n",
            " \u001b[90m [3bb67fe8] \u001b[39m\u001b[92m+ TranscodingStreams v0.9.6\u001b[39m\n",
            " \u001b[90m [ea10d353] \u001b[39m\u001b[92m+ WeakRefStrings v1.4.2\u001b[39m\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mInlineStrings\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mFilePathsBase\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mSentinelArrays\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mTranscodingStreams\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mCodecZlib\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mWeakRefStrings\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39mCSV\n",
            "  7 dependencies successfully precompiled in 9 seconds (157 already precompiled, 3 skipped during auto due to previous errors)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using CSV"
      ],
      "metadata": {
        "id": "iJxLB5RzVIEJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CSV.read(\n",
        "    download(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"), \n",
        "    DataFrame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "cZif5ZhHVM9X",
        "outputId": "7ca37a49-c628-4a8d-9fb7-70d0e3eef09c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[1m568×32 DataFrame\u001b[0m\n",
              "\u001b[1m Row \u001b[0m│\u001b[1m 842302   \u001b[0m\u001b[1m M       \u001b[0m\u001b[1m 17.99   \u001b[0m\u001b[1m 10.38   \u001b[0m\u001b[1m 122.8   \u001b[0m\u001b[1m 1001    \u001b[0m\u001b[1m 0.1184  \u001b[0m\u001b[1m 0.2776 \u001b[0m ⋯\n",
              "\u001b[1m     \u001b[0m│\u001b[90m Int64    \u001b[0m\u001b[90m String1 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64\u001b[0m ⋯\n",
              "─────┼──────────────────────────────────────────────────────────────────────────\n",
              "   1 │   842517  M         20.57     17.77   132.9    1326.0  0.08474  0.07864 ⋯\n",
              "   2 │ 84300903  M         19.69     21.25   130.0    1203.0  0.1096   0.1599\n",
              "   3 │ 84348301  M         11.42     20.38    77.58    386.1  0.1425   0.2839\n",
              "   4 │ 84358402  M         20.29     14.34   135.1    1297.0  0.1003   0.1328\n",
              "   5 │   843786  M         12.45     15.7     82.57    477.1  0.1278   0.17    ⋯\n",
              "   6 │   844359  M         18.25     19.98   119.6    1040.0  0.09463  0.109\n",
              "   7 │ 84458202  M         13.71     20.83    90.2     577.9  0.1189   0.1645\n",
              "   8 │   844981  M         13.0      21.82    87.5     519.8  0.1273   0.1932\n",
              "   9 │ 84501001  M         12.46     24.04    83.97    475.9  0.1186   0.2396  ⋯\n",
              "  10 │   845636  M         16.02     23.24   102.7     797.8  0.08206  0.06669\n",
              "  11 │ 84610002  M         15.78     17.89   103.6     781.0  0.0971   0.1292\n",
              "  ⋮  │    ⋮         ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮    ⋱\n",
              " 559 │   925291  B         11.51     23.93    74.52    403.5  0.09261  0.1021\n",
              " 560 │   925292  B         14.05     27.15    91.38    600.4  0.09929  0.1126  ⋯\n",
              " 561 │   925311  B         11.2      29.37    70.67    386.0  0.07449  0.03558\n",
              " 562 │   925622  M         15.22     30.62   103.4     716.9  0.1048   0.2087\n",
              " 563 │   926125  M         20.92     25.09   143.0    1347.0  0.1099   0.2236\n",
              " 564 │   926424  M         21.56     22.39   142.0    1479.0  0.111    0.1159  ⋯\n",
              " 565 │   926682  M         20.13     28.25   131.2    1261.0  0.0978   0.1034\n",
              " 566 │   926954  M         16.6      28.08   108.3     858.1  0.08455  0.1023\n",
              " 567 │   927241  M         20.6      29.33   140.1    1265.0  0.1178   0.277\n",
              " 568 │    92751  B          7.76     24.54    47.92    181.0  0.05263  0.04362 ⋯\n",
              "\u001b[36m                                                 24 columns and 547 rows omitted\u001b[0m"
            ],
            "text/html": [
              "<div class=\"data-frame\"><p>568 rows × 32 columns (omitted printing of 23 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>842302</th><th>M</th><th>17.99</th><th>10.38</th><th>122.8</th><th>1001</th><th>0.1184</th><th>0.2776</th><th>0.3001</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"String1\">String1</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>842517</td><td>M</td><td>20.57</td><td>17.77</td><td>132.9</td><td>1326.0</td><td>0.08474</td><td>0.07864</td><td>0.0869</td></tr><tr><th>2</th><td>84300903</td><td>M</td><td>19.69</td><td>21.25</td><td>130.0</td><td>1203.0</td><td>0.1096</td><td>0.1599</td><td>0.1974</td></tr><tr><th>3</th><td>84348301</td><td>M</td><td>11.42</td><td>20.38</td><td>77.58</td><td>386.1</td><td>0.1425</td><td>0.2839</td><td>0.2414</td></tr><tr><th>4</th><td>84358402</td><td>M</td><td>20.29</td><td>14.34</td><td>135.1</td><td>1297.0</td><td>0.1003</td><td>0.1328</td><td>0.198</td></tr><tr><th>5</th><td>843786</td><td>M</td><td>12.45</td><td>15.7</td><td>82.57</td><td>477.1</td><td>0.1278</td><td>0.17</td><td>0.1578</td></tr><tr><th>6</th><td>844359</td><td>M</td><td>18.25</td><td>19.98</td><td>119.6</td><td>1040.0</td><td>0.09463</td><td>0.109</td><td>0.1127</td></tr><tr><th>7</th><td>84458202</td><td>M</td><td>13.71</td><td>20.83</td><td>90.2</td><td>577.9</td><td>0.1189</td><td>0.1645</td><td>0.09366</td></tr><tr><th>8</th><td>844981</td><td>M</td><td>13.0</td><td>21.82</td><td>87.5</td><td>519.8</td><td>0.1273</td><td>0.1932</td><td>0.1859</td></tr><tr><th>9</th><td>84501001</td><td>M</td><td>12.46</td><td>24.04</td><td>83.97</td><td>475.9</td><td>0.1186</td><td>0.2396</td><td>0.2273</td></tr><tr><th>10</th><td>845636</td><td>M</td><td>16.02</td><td>23.24</td><td>102.7</td><td>797.8</td><td>0.08206</td><td>0.06669</td><td>0.03299</td></tr><tr><th>11</th><td>84610002</td><td>M</td><td>15.78</td><td>17.89</td><td>103.6</td><td>781.0</td><td>0.0971</td><td>0.1292</td><td>0.09954</td></tr><tr><th>12</th><td>846226</td><td>M</td><td>19.17</td><td>24.8</td><td>132.4</td><td>1123.0</td><td>0.0974</td><td>0.2458</td><td>0.2065</td></tr><tr><th>13</th><td>846381</td><td>M</td><td>15.85</td><td>23.95</td><td>103.7</td><td>782.7</td><td>0.08401</td><td>0.1002</td><td>0.09938</td></tr><tr><th>14</th><td>84667401</td><td>M</td><td>13.73</td><td>22.61</td><td>93.6</td><td>578.3</td><td>0.1131</td><td>0.2293</td><td>0.2128</td></tr><tr><th>15</th><td>84799002</td><td>M</td><td>14.54</td><td>27.54</td><td>96.73</td><td>658.8</td><td>0.1139</td><td>0.1595</td><td>0.1639</td></tr><tr><th>16</th><td>848406</td><td>M</td><td>14.68</td><td>20.13</td><td>94.74</td><td>684.5</td><td>0.09867</td><td>0.072</td><td>0.07395</td></tr><tr><th>17</th><td>84862001</td><td>M</td><td>16.13</td><td>20.68</td><td>108.1</td><td>798.8</td><td>0.117</td><td>0.2022</td><td>0.1722</td></tr><tr><th>18</th><td>849014</td><td>M</td><td>19.81</td><td>22.15</td><td>130.0</td><td>1260.0</td><td>0.09831</td><td>0.1027</td><td>0.1479</td></tr><tr><th>19</th><td>8510426</td><td>B</td><td>13.54</td><td>14.36</td><td>87.46</td><td>566.3</td><td>0.09779</td><td>0.08129</td><td>0.06664</td></tr><tr><th>20</th><td>8510653</td><td>B</td><td>13.08</td><td>15.71</td><td>85.63</td><td>520.0</td><td>0.1075</td><td>0.127</td><td>0.04568</td></tr><tr><th>21</th><td>8510824</td><td>B</td><td>9.504</td><td>12.44</td><td>60.34</td><td>273.9</td><td>0.1024</td><td>0.06492</td><td>0.02956</td></tr><tr><th>22</th><td>8511133</td><td>M</td><td>15.34</td><td>14.26</td><td>102.5</td><td>704.4</td><td>0.1073</td><td>0.2135</td><td>0.2077</td></tr><tr><th>23</th><td>851509</td><td>M</td><td>21.16</td><td>23.04</td><td>137.2</td><td>1404.0</td><td>0.09428</td><td>0.1022</td><td>0.1097</td></tr><tr><th>24</th><td>852552</td><td>M</td><td>16.65</td><td>21.38</td><td>110.0</td><td>904.6</td><td>0.1121</td><td>0.1457</td><td>0.1525</td></tr><tr><th>25</th><td>852631</td><td>M</td><td>17.14</td><td>16.4</td><td>116.0</td><td>912.7</td><td>0.1186</td><td>0.2276</td><td>0.2229</td></tr><tr><th>26</th><td>852763</td><td>M</td><td>14.58</td><td>21.53</td><td>97.41</td><td>644.8</td><td>0.1054</td><td>0.1868</td><td>0.1425</td></tr><tr><th>27</th><td>852781</td><td>M</td><td>18.61</td><td>20.25</td><td>122.1</td><td>1094.0</td><td>0.0944</td><td>0.1066</td><td>0.149</td></tr><tr><th>28</th><td>852973</td><td>M</td><td>15.3</td><td>25.27</td><td>102.4</td><td>732.4</td><td>0.1082</td><td>0.1697</td><td>0.1683</td></tr><tr><th>29</th><td>853201</td><td>M</td><td>17.57</td><td>15.05</td><td>115.0</td><td>955.1</td><td>0.09847</td><td>0.1157</td><td>0.09875</td></tr><tr><th>30</th><td>853401</td><td>M</td><td>18.63</td><td>25.11</td><td>124.8</td><td>1088.0</td><td>0.1064</td><td>0.1887</td><td>0.2319</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
            ],
            "text/latex": "\\begin{tabular}{r|cccccccccc}\n\t& 842302 & M & 17.99 & 10.38 & 122.8 & 1001 & 0.1184 & 0.2776 & 0.3001 & \\\\\n\t\\hline\n\t& Int64 & String1 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n\t\\hline\n\t1 & 842517 & M & 20.57 & 17.77 & 132.9 & 1326.0 & 0.08474 & 0.07864 & 0.0869 & $\\dots$ \\\\\n\t2 & 84300903 & M & 19.69 & 21.25 & 130.0 & 1203.0 & 0.1096 & 0.1599 & 0.1974 & $\\dots$ \\\\\n\t3 & 84348301 & M & 11.42 & 20.38 & 77.58 & 386.1 & 0.1425 & 0.2839 & 0.2414 & $\\dots$ \\\\\n\t4 & 84358402 & M & 20.29 & 14.34 & 135.1 & 1297.0 & 0.1003 & 0.1328 & 0.198 & $\\dots$ \\\\\n\t5 & 843786 & M & 12.45 & 15.7 & 82.57 & 477.1 & 0.1278 & 0.17 & 0.1578 & $\\dots$ \\\\\n\t6 & 844359 & M & 18.25 & 19.98 & 119.6 & 1040.0 & 0.09463 & 0.109 & 0.1127 & $\\dots$ \\\\\n\t7 & 84458202 & M & 13.71 & 20.83 & 90.2 & 577.9 & 0.1189 & 0.1645 & 0.09366 & $\\dots$ \\\\\n\t8 & 844981 & M & 13.0 & 21.82 & 87.5 & 519.8 & 0.1273 & 0.1932 & 0.1859 & $\\dots$ \\\\\n\t9 & 84501001 & M & 12.46 & 24.04 & 83.97 & 475.9 & 0.1186 & 0.2396 & 0.2273 & $\\dots$ \\\\\n\t10 & 845636 & M & 16.02 & 23.24 & 102.7 & 797.8 & 0.08206 & 0.06669 & 0.03299 & $\\dots$ \\\\\n\t11 & 84610002 & M & 15.78 & 17.89 & 103.6 & 781.0 & 0.0971 & 0.1292 & 0.09954 & $\\dots$ \\\\\n\t12 & 846226 & M & 19.17 & 24.8 & 132.4 & 1123.0 & 0.0974 & 0.2458 & 0.2065 & $\\dots$ \\\\\n\t13 & 846381 & M & 15.85 & 23.95 & 103.7 & 782.7 & 0.08401 & 0.1002 & 0.09938 & $\\dots$ \\\\\n\t14 & 84667401 & M & 13.73 & 22.61 & 93.6 & 578.3 & 0.1131 & 0.2293 & 0.2128 & $\\dots$ \\\\\n\t15 & 84799002 & M & 14.54 & 27.54 & 96.73 & 658.8 & 0.1139 & 0.1595 & 0.1639 & $\\dots$ \\\\\n\t16 & 848406 & M & 14.68 & 20.13 & 94.74 & 684.5 & 0.09867 & 0.072 & 0.07395 & $\\dots$ \\\\\n\t17 & 84862001 & M & 16.13 & 20.68 & 108.1 & 798.8 & 0.117 & 0.2022 & 0.1722 & $\\dots$ \\\\\n\t18 & 849014 & M & 19.81 & 22.15 & 130.0 & 1260.0 & 0.09831 & 0.1027 & 0.1479 & $\\dots$ \\\\\n\t19 & 8510426 & B & 13.54 & 14.36 & 87.46 & 566.3 & 0.09779 & 0.08129 & 0.06664 & $\\dots$ \\\\\n\t20 & 8510653 & B & 13.08 & 15.71 & 85.63 & 520.0 & 0.1075 & 0.127 & 0.04568 & $\\dots$ \\\\\n\t21 & 8510824 & B & 9.504 & 12.44 & 60.34 & 273.9 & 0.1024 & 0.06492 & 0.02956 & $\\dots$ \\\\\n\t22 & 8511133 & M & 15.34 & 14.26 & 102.5 & 704.4 & 0.1073 & 0.2135 & 0.2077 & $\\dots$ \\\\\n\t23 & 851509 & M & 21.16 & 23.04 & 137.2 & 1404.0 & 0.09428 & 0.1022 & 0.1097 & $\\dots$ \\\\\n\t24 & 852552 & M & 16.65 & 21.38 & 110.0 & 904.6 & 0.1121 & 0.1457 & 0.1525 & $\\dots$ \\\\\n\t25 & 852631 & M & 17.14 & 16.4 & 116.0 & 912.7 & 0.1186 & 0.2276 & 0.2229 & $\\dots$ \\\\\n\t26 & 852763 & M & 14.58 & 21.53 & 97.41 & 644.8 & 0.1054 & 0.1868 & 0.1425 & $\\dots$ \\\\\n\t27 & 852781 & M & 18.61 & 20.25 & 122.1 & 1094.0 & 0.0944 & 0.1066 & 0.149 & $\\dots$ \\\\\n\t28 & 852973 & M & 15.3 & 25.27 & 102.4 & 732.4 & 0.1082 & 0.1697 & 0.1683 & $\\dots$ \\\\\n\t29 & 853201 & M & 17.57 & 15.05 & 115.0 & 955.1 & 0.09847 & 0.1157 & 0.09875 & $\\dots$ \\\\\n\t30 & 853401 & M & 18.63 & 25.11 & 124.8 & 1088.0 & 0.1064 & 0.1887 & 0.2319 & $\\dots$ \\\\\n\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n\\end{tabular}\n"
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(numrows,numcolumns) = size(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FcjZBjqWtEi",
        "outputId": "1d1269a7-3ee9-44e9-fc43-0ad478592599"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(568, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number Malignant\n",
        "forCnt = dataset[:,[:M]]\n",
        "size(filter(row -> row.M ==\"M\", forCnt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-18AG5gcRTZ",
        "outputId": "752907fb-1ae4-481f-e641-d2b4224dd9ce"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(211, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number Benign\n",
        "size(filter(row -> row.M ==\"B\", forCnt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68103bab-67bb-4f5c-8bef-df6868e8178c",
        "id": "QCKWF-EZhc8o"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(357, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "describe(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "IHbuBrSeXw7b",
        "outputId": "424845ee-48c1-45c5-febc-85ea0b6845b4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[1m32×7 DataFrame\u001b[0m\n",
              "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean       \u001b[0m\u001b[1m min       \u001b[0m\u001b[1m median    \u001b[0m\u001b[1m max       \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype\u001b[0m ⋯\n",
              "\u001b[1m     \u001b[0m│\u001b[90m Symbol   \u001b[0m\u001b[90m Union…     \u001b[0m\u001b[90m Any       \u001b[0m\u001b[90m Union…    \u001b[0m\u001b[90m Any       \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataTy\u001b[0m ⋯\n",
              "─────┼──────────────────────────────────────────────────────────────────────────\n",
              "   1 │ 842302    3.04238e7   8670       906157.0   911320502         0  Int64  ⋯\n",
              "   2 │ M        \u001b[90m            \u001b[0m B         \u001b[90m           \u001b[0m M                 0  String\n",
              "   3 │ 17.99     14.1205     6.981      13.355     28.11             0  Float6\n",
              "   4 │ 10.38     19.3053     9.71       18.855     39.28             0  Float6\n",
              "   5 │ 122.8     91.9148     43.79      86.21      188.5             0  Float6 ⋯\n",
              "   6 │ 1001      654.28      143.5      548.75     2501.0            0  Float6\n",
              "   7 │ 0.1184    0.0963215   0.05263    0.095865   0.1634            0  Float6\n",
              "   8 │ 0.2776    0.104036    0.01938    0.092525   0.3454            0  Float6\n",
              "   9 │ 0.3001    0.0884273   0.0        0.0614     0.4268            0  Float6 ⋯\n",
              "  10 │ 0.1471    0.0487463   0.0        0.033455   0.2012            0  Float6\n",
              "  11 │ 0.2419    0.181055    0.106      0.1792     0.304             0  Float6\n",
              "  ⋮  │    ⋮          ⋮           ⋮          ⋮          ⋮         ⋮         ⋮   ⋱\n",
              "  23 │ 25.38     16.2531     7.93       14.965     36.04             0  Float6\n",
              "  24 │ 17.33     25.6919     12.02      25.425     49.54             0  Float6 ⋯\n",
              "  25 │ 184.6     107.125     50.41      97.655     251.2             0  Float6\n",
              "  26 │ 2019      878.579     185.2      685.55     4254.0            0  Float6\n",
              "  27 │ 0.1622    0.132316    0.07117    0.1313     0.2226            0  Float6\n",
              "  28 │ 0.6656    0.253541    0.02729    0.21185    1.058             0  Float6 ⋯\n",
              "  29 │ 0.7119    0.271414    0.0        0.22655    1.252             0  Float6\n",
              "  30 │ 0.2654    0.114341    0.0        0.09984    0.291             0  Float6\n",
              "  31 │ 0.4601    0.289776    0.1565     0.28205    0.6638            0  Float6\n",
              "  32 │ 0.1189    0.0838843   0.05504    0.080015   0.2075            0  Float6 ⋯\n",
              "\u001b[36m                                                    1 column and 11 rows omitted\u001b[0m"
            ],
            "text/html": [
              "<div class=\"data-frame\"><p>32 rows × 7 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title=\"Symbol\">Symbol</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Int64\">Int64</th><th title=\"DataType\">DataType</th></tr></thead><tbody><tr><th>1</th><td>842302</td><td>3.04238e7</td><td>8670</td><td>906157.0</td><td>911320502</td><td>0</td><td>Int64</td></tr><tr><th>2</th><td>M</td><td></td><td>B</td><td></td><td>M</td><td>0</td><td>String1</td></tr><tr><th>3</th><td>17.99</td><td>14.1205</td><td>6.981</td><td>13.355</td><td>28.11</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>10.38</td><td>19.3053</td><td>9.71</td><td>18.855</td><td>39.28</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>122.8</td><td>91.9148</td><td>43.79</td><td>86.21</td><td>188.5</td><td>0</td><td>Float64</td></tr><tr><th>6</th><td>1001</td><td>654.28</td><td>143.5</td><td>548.75</td><td>2501.0</td><td>0</td><td>Float64</td></tr><tr><th>7</th><td>0.1184</td><td>0.0963215</td><td>0.05263</td><td>0.095865</td><td>0.1634</td><td>0</td><td>Float64</td></tr><tr><th>8</th><td>0.2776</td><td>0.104036</td><td>0.01938</td><td>0.092525</td><td>0.3454</td><td>0</td><td>Float64</td></tr><tr><th>9</th><td>0.3001</td><td>0.0884273</td><td>0.0</td><td>0.0614</td><td>0.4268</td><td>0</td><td>Float64</td></tr><tr><th>10</th><td>0.1471</td><td>0.0487463</td><td>0.0</td><td>0.033455</td><td>0.2012</td><td>0</td><td>Float64</td></tr><tr><th>11</th><td>0.2419</td><td>0.181055</td><td>0.106</td><td>0.1792</td><td>0.304</td><td>0</td><td>Float64</td></tr><tr><th>12</th><td>0.07871</td><td>0.0627696</td><td>0.04996</td><td>0.061515</td><td>0.09744</td><td>0</td><td>Float64</td></tr><tr><th>13</th><td>1.095</td><td>0.403958</td><td>0.1115</td><td>0.32395</td><td>2.873</td><td>0</td><td>Float64</td></tr><tr><th>14</th><td>0.9053</td><td>1.2174</td><td>0.3602</td><td>1.1095</td><td>4.885</td><td>0</td><td>Float64</td></tr><tr><th>15</th><td>8.589</td><td>2.85598</td><td>0.757</td><td>2.2855</td><td>21.98</td><td>0</td><td>Float64</td></tr><tr><th>16</th><td>153.4</td><td>40.138</td><td>6.802</td><td>24.485</td><td>542.2</td><td>0</td><td>Float64</td></tr><tr><th>17</th><td>0.006399</td><td>0.00704211</td><td>0.001713</td><td>0.0063745</td><td>0.03113</td><td>0</td><td>Float64</td></tr><tr><th>18</th><td>0.04904</td><td>0.0254367</td><td>0.002252</td><td>0.020435</td><td>0.1354</td><td>0</td><td>Float64</td></tr><tr><th>19</th><td>0.05373</td><td>0.0318553</td><td>0.0</td><td>0.025875</td><td>0.396</td><td>0</td><td>Float64</td></tr><tr><th>20</th><td>0.01587</td><td>0.011789</td><td>0.0</td><td>0.01092</td><td>0.05279</td><td>0</td><td>Float64</td></tr><tr><th>21</th><td>0.03003</td><td>0.0205256</td><td>0.007882</td><td>0.018725</td><td>0.07895</td><td>0</td><td>Float64</td></tr><tr><th>22</th><td>0.006193</td><td>0.00379068</td><td>0.0008948</td><td>0.0031615</td><td>0.02984</td><td>0</td><td>Float64</td></tr><tr><th>23</th><td>25.38</td><td>16.2531</td><td>7.93</td><td>14.965</td><td>36.04</td><td>0</td><td>Float64</td></tr><tr><th>24</th><td>17.33</td><td>25.6919</td><td>12.02</td><td>25.425</td><td>49.54</td><td>0</td><td>Float64</td></tr><tr><th>25</th><td>184.6</td><td>107.125</td><td>50.41</td><td>97.655</td><td>251.2</td><td>0</td><td>Float64</td></tr><tr><th>26</th><td>2019</td><td>878.579</td><td>185.2</td><td>685.55</td><td>4254.0</td><td>0</td><td>Float64</td></tr><tr><th>27</th><td>0.1622</td><td>0.132316</td><td>0.07117</td><td>0.1313</td><td>0.2226</td><td>0</td><td>Float64</td></tr><tr><th>28</th><td>0.6656</td><td>0.253541</td><td>0.02729</td><td>0.21185</td><td>1.058</td><td>0</td><td>Float64</td></tr><tr><th>29</th><td>0.7119</td><td>0.271414</td><td>0.0</td><td>0.22655</td><td>1.252</td><td>0</td><td>Float64</td></tr><tr><th>30</th><td>0.2654</td><td>0.114341</td><td>0.0</td><td>0.09984</td><td>0.291</td><td>0</td><td>Float64</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
            ],
            "text/latex": "\\begin{tabular}{r|ccccccc}\n\t& variable & mean & min & median & max & nmissing & eltype\\\\\n\t\\hline\n\t& Symbol & Union… & Any & Union… & Any & Int64 & DataType\\\\\n\t\\hline\n\t1 & 842302 & 3.04238e7 & 8670 & 906157.0 & 911320502 & 0 & Int64 \\\\\n\t2 & M &  & B &  & M & 0 & String1 \\\\\n\t3 & 17.99 & 14.1205 & 6.981 & 13.355 & 28.11 & 0 & Float64 \\\\\n\t4 & 10.38 & 19.3053 & 9.71 & 18.855 & 39.28 & 0 & Float64 \\\\\n\t5 & 122.8 & 91.9148 & 43.79 & 86.21 & 188.5 & 0 & Float64 \\\\\n\t6 & 1001 & 654.28 & 143.5 & 548.75 & 2501.0 & 0 & Float64 \\\\\n\t7 & 0.1184 & 0.0963215 & 0.05263 & 0.095865 & 0.1634 & 0 & Float64 \\\\\n\t8 & 0.2776 & 0.104036 & 0.01938 & 0.092525 & 0.3454 & 0 & Float64 \\\\\n\t9 & 0.3001 & 0.0884273 & 0.0 & 0.0614 & 0.4268 & 0 & Float64 \\\\\n\t10 & 0.1471 & 0.0487463 & 0.0 & 0.033455 & 0.2012 & 0 & Float64 \\\\\n\t11 & 0.2419 & 0.181055 & 0.106 & 0.1792 & 0.304 & 0 & Float64 \\\\\n\t12 & 0.07871 & 0.0627696 & 0.04996 & 0.061515 & 0.09744 & 0 & Float64 \\\\\n\t13 & 1.095 & 0.403958 & 0.1115 & 0.32395 & 2.873 & 0 & Float64 \\\\\n\t14 & 0.9053 & 1.2174 & 0.3602 & 1.1095 & 4.885 & 0 & Float64 \\\\\n\t15 & 8.589 & 2.85598 & 0.757 & 2.2855 & 21.98 & 0 & Float64 \\\\\n\t16 & 153.4 & 40.138 & 6.802 & 24.485 & 542.2 & 0 & Float64 \\\\\n\t17 & 0.006399 & 0.00704211 & 0.001713 & 0.0063745 & 0.03113 & 0 & Float64 \\\\\n\t18 & 0.04904 & 0.0254367 & 0.002252 & 0.020435 & 0.1354 & 0 & Float64 \\\\\n\t19 & 0.05373 & 0.0318553 & 0.0 & 0.025875 & 0.396 & 0 & Float64 \\\\\n\t20 & 0.01587 & 0.011789 & 0.0 & 0.01092 & 0.05279 & 0 & Float64 \\\\\n\t21 & 0.03003 & 0.0205256 & 0.007882 & 0.018725 & 0.07895 & 0 & Float64 \\\\\n\t22 & 0.006193 & 0.00379068 & 0.0008948 & 0.0031615 & 0.02984 & 0 & Float64 \\\\\n\t23 & 25.38 & 16.2531 & 7.93 & 14.965 & 36.04 & 0 & Float64 \\\\\n\t24 & 17.33 & 25.6919 & 12.02 & 25.425 & 49.54 & 0 & Float64 \\\\\n\t25 & 184.6 & 107.125 & 50.41 & 97.655 & 251.2 & 0 & Float64 \\\\\n\t26 & 2019 & 878.579 & 185.2 & 685.55 & 4254.0 & 0 & Float64 \\\\\n\t27 & 0.1622 & 0.132316 & 0.07117 & 0.1313 & 0.2226 & 0 & Float64 \\\\\n\t28 & 0.6656 & 0.253541 & 0.02729 & 0.21185 & 1.058 & 0 & Float64 \\\\\n\t29 & 0.7119 & 0.271414 & 0.0 & 0.22655 & 1.252 & 0 & Float64 \\\\\n\t30 & 0.2654 & 0.114341 & 0.0 & 0.09984 & 0.291 & 0 & Float64 \\\\\n\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n\\end{tabular}\n"
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "C8w-_j3KtUza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the dataframe into an x array for features and the y vector for\n",
        " results of benign or malignant. Additionally, convert the array of strings \"B\" and \"M\" into integer values 0 and 1 respectively:"
      ],
      "metadata": {
        "id": "ThnBCjoptgHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = Matrix(dataset[:,3:32])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZefjxXjte3R",
        "outputId": "6ff7185a-b537-4dec-a195-2878ec1df2bd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "568×30 Matrix{Float64}:\n",
              " 20.57   17.77  132.9   1326.0  0.08474  …  0.2416  0.186    0.275   0.08902\n",
              " 19.69   21.25  130.0   1203.0  0.1096      0.4504  0.243    0.3613  0.08758\n",
              " 11.42   20.38   77.58   386.1  0.1425      0.6869  0.2575   0.6638  0.173\n",
              " 20.29   14.34  135.1   1297.0  0.1003      0.4     0.1625   0.2364  0.07678\n",
              " 12.45   15.7    82.57   477.1  0.1278      0.5355  0.1741   0.3985  0.1244\n",
              " 18.25   19.98  119.6   1040.0  0.09463  …  0.3784  0.1932   0.3063  0.08368\n",
              " 13.71   20.83   90.2    577.9  0.1189      0.2678  0.1556   0.3196  0.1151\n",
              " 13.0    21.82   87.5    519.8  0.1273      0.539   0.206    0.4378  0.1072\n",
              " 12.46   24.04   83.97   475.9  0.1186      1.105   0.221    0.4366  0.2075\n",
              " 16.02   23.24  102.7    797.8  0.08206     0.1459  0.09975  0.2948  0.08452\n",
              " 15.78   17.89  103.6    781.0  0.0971   …  0.3965  0.181    0.3792  0.1048\n",
              " 19.17   24.8   132.4   1123.0  0.0974      0.3639  0.1767   0.3176  0.1023\n",
              " 15.85   23.95  103.7    782.7  0.08401     0.2322  0.1119   0.2809  0.06287\n",
              "  ⋮                                      ⋱                           \n",
              "  9.423  27.88   59.26   271.3  0.08123     0.0     0.0      0.2475  0.06969\n",
              " 14.59   22.68   96.39   657.1  0.08473     0.3662  0.1105   0.2258  0.08004\n",
              " 11.51   23.93   74.52   403.5  0.09261     0.363   0.09653  0.2112  0.08732\n",
              " 14.05   27.15   91.38   600.4  0.09929     0.1326  0.1048   0.225   0.08321\n",
              " 11.2    29.37   70.67   386.0  0.07449  …  0.0     0.0      0.1566  0.05905\n",
              " 15.22   30.62  103.4    716.9  0.1048      1.17    0.2356   0.4089  0.1409\n",
              " 20.92   25.09  143.0   1347.0  0.1099      0.6599  0.2542   0.2929  0.09873\n",
              " 21.56   22.39  142.0   1479.0  0.111       0.4107  0.2216   0.206   0.07115\n",
              " 20.13   28.25  131.2   1261.0  0.0978      0.3215  0.1628   0.2572  0.06637\n",
              " 16.6    28.08  108.3    858.1  0.08455  …  0.3403  0.1418   0.2218  0.0782\n",
              " 20.6    29.33  140.1   1265.0  0.1178      0.9387  0.265    0.4087  0.124\n",
              "  7.76   24.54   47.92   181.0  0.05263     0.0     0.0      0.2871  0.07039"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "typeof(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbORVjcN0Vt_",
        "outputId": "4a1775a3-8b47-47a8-b90f-8b8df17cf750"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Matrix{Float64} (alias for Array{Float64, 2})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y = Vector(map(element -> element == \"B\" ? 0 : 1, dataset[!,:M]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwsRPm5XtetI",
        "outputId": "49a3b84e-89fb-40b5-d8f1-190c1124cf5b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "568-element Vector{Int64}:\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " ⋮\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 0"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "typeof(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asF6j-0UzrWH",
        "outputId": "df97756b-355f-4bcc-840f-cc9a6cf9a812"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vector{Int64} (alias for Array{Int64, 1})"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataframes x, y to Arrays x, y\n",
        "Matrix(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6UtAmShtebH",
        "outputId": "d9c7501c-de44-48e4-f2d2-7053a4ae5a33"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "568×30 Matrix{Float64}:\n",
              " 20.57   17.77  132.9   1326.0  0.08474  …  0.2416  0.186    0.275   0.08902\n",
              " 19.69   21.25  130.0   1203.0  0.1096      0.4504  0.243    0.3613  0.08758\n",
              " 11.42   20.38   77.58   386.1  0.1425      0.6869  0.2575   0.6638  0.173\n",
              " 20.29   14.34  135.1   1297.0  0.1003      0.4     0.1625   0.2364  0.07678\n",
              " 12.45   15.7    82.57   477.1  0.1278      0.5355  0.1741   0.3985  0.1244\n",
              " 18.25   19.98  119.6   1040.0  0.09463  …  0.3784  0.1932   0.3063  0.08368\n",
              " 13.71   20.83   90.2    577.9  0.1189      0.2678  0.1556   0.3196  0.1151\n",
              " 13.0    21.82   87.5    519.8  0.1273      0.539   0.206    0.4378  0.1072\n",
              " 12.46   24.04   83.97   475.9  0.1186      1.105   0.221    0.4366  0.2075\n",
              " 16.02   23.24  102.7    797.8  0.08206     0.1459  0.09975  0.2948  0.08452\n",
              " 15.78   17.89  103.6    781.0  0.0971   …  0.3965  0.181    0.3792  0.1048\n",
              " 19.17   24.8   132.4   1123.0  0.0974      0.3639  0.1767   0.3176  0.1023\n",
              " 15.85   23.95  103.7    782.7  0.08401     0.2322  0.1119   0.2809  0.06287\n",
              "  ⋮                                      ⋱                           \n",
              "  9.423  27.88   59.26   271.3  0.08123     0.0     0.0      0.2475  0.06969\n",
              " 14.59   22.68   96.39   657.1  0.08473     0.3662  0.1105   0.2258  0.08004\n",
              " 11.51   23.93   74.52   403.5  0.09261     0.363   0.09653  0.2112  0.08732\n",
              " 14.05   27.15   91.38   600.4  0.09929     0.1326  0.1048   0.225   0.08321\n",
              " 11.2    29.37   70.67   386.0  0.07449  …  0.0     0.0      0.1566  0.05905\n",
              " 15.22   30.62  103.4    716.9  0.1048      1.17    0.2356   0.4089  0.1409\n",
              " 20.92   25.09  143.0   1347.0  0.1099      0.6599  0.2542   0.2929  0.09873\n",
              " 21.56   22.39  142.0   1479.0  0.111       0.4107  0.2216   0.206   0.07115\n",
              " 20.13   28.25  131.2   1261.0  0.0978      0.3215  0.1628   0.2572  0.06637\n",
              " 16.6    28.08  108.3    858.1  0.08455  …  0.3403  0.1418   0.2218  0.0782\n",
              " 20.6    29.33  140.1   1265.0  0.1178      0.9387  0.265    0.4087  0.124\n",
              "  7.76   24.54   47.92   181.0  0.05263     0.0     0.0      0.2871  0.07039"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import Pkg; Pkg.add(\"MLDataUtils\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm6F_0a32X1q",
        "outputId": "ed4830fa-3cec-4725-cea2-82a73ab52987"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MappedArrays ── v0.4.1\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LearnBase ───── v0.3.0\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLLabelUtils ── v0.5.7\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLDataPattern ─ v0.5.4\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLDataUtils ─── v0.5.4\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Project.toml`\n",
            " \u001b[90m [cc2ba9b6] \u001b[39m\u001b[92m+ MLDataUtils v0.5.4\u001b[39m\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Manifest.toml`\n",
            " \u001b[90m [7f8f8fb0] \u001b[39m\u001b[92m+ LearnBase v0.3.0\u001b[39m\n",
            " \u001b[90m [9920b226] \u001b[39m\u001b[92m+ MLDataPattern v0.5.4\u001b[39m\n",
            " \u001b[90m [cc2ba9b6] \u001b[39m\u001b[92m+ MLDataUtils v0.5.4\u001b[39m\n",
            " \u001b[90m [66a33bbf] \u001b[39m\u001b[92m+ MLLabelUtils v0.5.7\u001b[39m\n",
            " \u001b[90m [dbb5928d] \u001b[39m\u001b[92m+ MappedArrays v0.4.1\u001b[39m\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mMappedArrays\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mLearnBase\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mMLLabelUtils\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataPattern\u001b[39m\n",
            "\u001b[32m  ✓ \u001b[39mMLDataUtils\n",
            "  5 dependencies successfully precompiled in 5 seconds (164 already precompiled, 3 skipped during auto due to previous errors)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using MLDataUtils"
      ],
      "metadata": {
        "id": "gIvfCcjc2r1m"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomize the data rows so we don't pull only one cancer classification.\n",
        "Transpose(x) so it properly aligns with the output dimensions"
      ],
      "metadata": {
        "id": "kymyfXuK4DmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xs, Ys = shuffleobs((transpose(x), y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6chRmUM3tNi",
        "outputId": "9941d51f-962b-4425-efed-388ae18e2dbf"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([17.05 14.61 … 12.56 12.23; 19.08 15.69 … 19.07 19.56; … ; 0.3109 0.253 … 0.2121 0.2668; 0.09061 0.05695 … 0.07188 0.08174], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0  …  0, 0, 0, 1, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split data into training and test sets {2/3 training; 1/3 test}"
      ],
      "metadata": {
        "id": "whw2H8aE4hq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train1, y_train1), (X_test1, y_test1) = splitobs((Xs, Ys); at = 0.67)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij75RG-y4im_",
        "outputId": "0aeebc66-9b95-4e91-e29f-594b7dfdceea"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(([17.05 14.61 … 13.96 13.0; 19.08 15.69 … 17.05 21.82; … ; 0.3109 0.253 … 0.3068 0.4378; 0.09061 0.05695 … 0.07957 0.1072], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0  …  1, 0, 0, 1, 0, 0, 0, 1, 1, 1]), ([12.32 18.61 … 12.56 12.23; 12.39 20.25 … 19.07 19.56; … ; 0.2827 0.2341 … 0.2121 0.2668; 0.06771 0.07421 … 0.07188 0.08174], [0, 1, 0, 1, 1, 0, 1, 0, 1, 0  …  0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transpose x back \n",
        "    x_train = Array(transpose(X_train1))\n",
        "    y_train = Array(y_train1)\n",
        "    x_test = Array(transpose(X_test1))\n",
        "    y_test = Array(y_test1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWuGIsMl4iYe",
        "outputId": "c2df1d16-b4f5-4f7b-cb52-30044775b33b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "187-element Vector{Int64}:\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 1\n",
              " ⋮\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters for Tunning**\n",
        "\n",
        "eta [default=0.3, alias: learning_rate]\n",
        "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
        "range: [0,1]\n",
        "\n",
        "max_depth [default=6]\n",
        "Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. exact tree method requires non-zero value.\n",
        "range: [0,∞]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YfbmLLX6DTUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtrain = DMatrix(x_train, label = y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQD6k9oo6PFg",
        "outputId": "1a77b57b-bed7-439b-9842-3538099c3526"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DMatrix(Ptr{Nothing} @0x0000000015a0d960, XGBoost.var\"#_setinfo#8\"())"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boost = xgboost(dtrain, num_round = 2, 100, eta = 1 , objective = \"binary:logistic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVOke9al6ZS2",
        "outputId": "7f6732ef-d5e5-422e-8bb8-925253dd14c2"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[22:37:54] WARNING: /workspace/srcdir/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[1]\ttrain-logloss:0.186739\n",
            "[2]\ttrain-logloss:0.090753\n",
            "[3]\ttrain-logloss:0.053734\n",
            "[4]\ttrain-logloss:0.035809\n",
            "[5]\ttrain-logloss:0.027452\n",
            "[6]\ttrain-logloss:0.021554\n",
            "[7]\ttrain-logloss:0.017867\n",
            "[8]\ttrain-logloss:0.015599\n",
            "[9]\ttrain-logloss:0.012860\n",
            "[10]\ttrain-logloss:0.011259\n",
            "[11]\ttrain-logloss:0.009873\n",
            "[12]\ttrain-logloss:0.009150\n",
            "[13]\ttrain-logloss:0.008803\n",
            "[14]\ttrain-logloss:0.008533\n",
            "[15]\ttrain-logloss:0.008328\n",
            "[16]\ttrain-logloss:0.008059\n",
            "[17]\ttrain-logloss:0.007796\n",
            "[18]\ttrain-logloss:0.007525\n",
            "[19]\ttrain-logloss:0.007355\n",
            "[20]\ttrain-logloss:0.007127\n",
            "[21]\ttrain-logloss:0.006959\n",
            "[22]\ttrain-logloss:0.006815\n",
            "[23]\ttrain-logloss:0.006647\n",
            "[24]\ttrain-logloss:0.006532\n",
            "[25]\ttrain-logloss:0.006415\n",
            "[26]\ttrain-logloss:0.006326\n",
            "[27]\ttrain-logloss:0.006224\n",
            "[28]\ttrain-logloss:0.006145\n",
            "[29]\ttrain-logloss:0.006067\n",
            "[30]\ttrain-logloss:0.005999\n",
            "[31]\ttrain-logloss:0.005933\n",
            "[32]\ttrain-logloss:0.005890\n",
            "[33]\ttrain-logloss:0.005825\n",
            "[34]\ttrain-logloss:0.005786\n",
            "[35]\ttrain-logloss:0.005737\n",
            "[36]\ttrain-logloss:0.005695\n",
            "[37]\ttrain-logloss:0.005667\n",
            "[38]\ttrain-logloss:0.005667\n",
            "[39]\ttrain-logloss:0.005667\n",
            "[40]\ttrain-logloss:0.005667\n",
            "[41]\ttrain-logloss:0.005667\n",
            "[42]\ttrain-logloss:0.005667\n",
            "[43]\ttrain-logloss:0.005667\n",
            "[44]\ttrain-logloss:0.005667\n",
            "[45]\ttrain-logloss:0.005667\n",
            "[46]\ttrain-logloss:0.005667\n",
            "[47]\ttrain-logloss:0.005667\n",
            "[48]\ttrain-logloss:0.005667\n",
            "[49]\ttrain-logloss:0.005667\n",
            "[50]\ttrain-logloss:0.005667\n",
            "[51]\ttrain-logloss:0.005667\n",
            "[52]\ttrain-logloss:0.005667\n",
            "[53]\ttrain-logloss:0.005667\n",
            "[54]\ttrain-logloss:0.005667\n",
            "[55]\ttrain-logloss:0.005667\n",
            "[56]\ttrain-logloss:0.005667\n",
            "[57]\ttrain-logloss:0.005667\n",
            "[58]\ttrain-logloss:0.005667\n",
            "[59]\ttrain-logloss:0.005667\n",
            "[60]\ttrain-logloss:0.005667\n",
            "[61]\ttrain-logloss:0.005667\n",
            "[62]\ttrain-logloss:0.005667\n",
            "[63]\ttrain-logloss:0.005667\n",
            "[64]\ttrain-logloss:0.005667\n",
            "[65]\ttrain-logloss:0.005667\n",
            "[66]\ttrain-logloss:0.005667\n",
            "[67]\ttrain-logloss:0.005667\n",
            "[68]\ttrain-logloss:0.005667\n",
            "[69]\ttrain-logloss:0.005667\n",
            "[70]\ttrain-logloss:0.005667\n",
            "[71]\ttrain-logloss:0.005667\n",
            "[72]\ttrain-logloss:0.005667\n",
            "[73]\ttrain-logloss:0.005667\n",
            "[74]\ttrain-logloss:0.005667\n",
            "[75]\ttrain-logloss:0.005667\n",
            "[76]\ttrain-logloss:0.005667\n",
            "[77]\ttrain-logloss:0.005667\n",
            "[78]\ttrain-logloss:0.005667\n",
            "[79]\ttrain-logloss:0.005667\n",
            "[80]\ttrain-logloss:0.005667\n",
            "[81]\ttrain-logloss:0.005667\n",
            "[82]\ttrain-logloss:0.005667\n",
            "[83]\ttrain-logloss:0.005667\n",
            "[84]\ttrain-logloss:0.005667\n",
            "[85]\ttrain-logloss:0.005667\n",
            "[86]\ttrain-logloss:0.005667\n",
            "[87]\ttrain-logloss:0.005667\n",
            "[88]\ttrain-logloss:0.005667\n",
            "[89]\ttrain-logloss:0.005667\n",
            "[90]\ttrain-logloss:0.005667\n",
            "[91]\ttrain-logloss:0.005667\n",
            "[92]\ttrain-logloss:0.005667\n",
            "[93]\ttrain-logloss:0.005667\n",
            "[94]\ttrain-logloss:0.005667\n",
            "[95]\ttrain-logloss:0.005667\n",
            "[96]\ttrain-logloss:0.005667\n",
            "[97]\ttrain-logloss:0.005667\n",
            "[98]\ttrain-logloss:0.005667\n",
            "[99]\ttrain-logloss:0.005667\n",
            "[100]\ttrain-logloss:0.005667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Booster(Ptr{Nothing} @0x0000000014b5a9c0)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = XGBoost.predict(boost, x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIVkMB_H7Bs1",
        "outputId": "732b8d4e-3cd2-4a80-8511-b147f23645d5"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "187-element Vector{Float32}:\n",
              " 0.000115474846\n",
              " 0.99954456\n",
              " 0.00017906365\n",
              " 0.99991167\n",
              " 0.9999027\n",
              " 0.0003317018\n",
              " 0.9996939\n",
              " 6.926588f-5\n",
              " 0.9999306\n",
              " 0.9237183\n",
              " 0.010344508\n",
              " 0.00014393381\n",
              " 0.9991441\n",
              " ⋮\n",
              " 0.0005576648\n",
              " 0.16607077\n",
              " 0.0001469575\n",
              " 7.55378f-5\n",
              " 0.00010939565\n",
              " 0.99980956\n",
              " 0.0002482798\n",
              " 0.70105135\n",
              " 0.0016226117\n",
              " 0.00013114279\n",
              " 0.0003866863\n",
              " 0.0046268953"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_rounded = Array{Int64, 1}(map(val -> round(val), prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxwOu8Ly7VmZ",
        "outputId": "2c05d8c7-8d48-44fb-d3c7-c64a44a31cfe"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "187-element Vector{Int64}:\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 1\n",
              " 0\n",
              " 0\n",
              " 1\n",
              " ⋮\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import Pkg; Pkg.add(\"MLBase\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EIlpmNt8SsG",
        "outputId": "c498f705-1291-4a43-a337-d37a9cbae576"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLBase ─ v0.9.0\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Project.toml`\n",
            " \u001b[90m [f0e99cf1] \u001b[39m\u001b[92m+ MLBase v0.9.0\u001b[39m\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Manifest.toml`\n",
            " \u001b[90m [f0e99cf1] \u001b[39m\u001b[92m+ MLBase v0.9.0\u001b[39m\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[32m  ✓ \u001b[39mMLBase\n",
            "  1 dependency successfully precompiled in 1 seconds (169 already precompiled, 3 skipped during auto due to previous errors)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using MLBase\n",
        "errorrate(y_test, prediction_rounded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZqBV2wU8Ebg",
        "outputId": "4188b968-f4d2-4a75-a2ea-d0134987b619"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.026737967914438502"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLBase.confusmat(2, Array{Int64, 1}(y_test .+1), Array{Int64,1}(prediction_rounded .+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2cGfbZ1BvWm",
        "outputId": "855c572e-bca3-4e71-e130-396fdbe5de88"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×2 Matrix{Int64}:\n",
              " 120   4\n",
              "   1  62"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boost = xgboost(dtrain, num_round = 2, 1000, eta = 1 , max_depth = 10, objective = \"binary:logistic\")\n",
        "prediction_rounded = Array{Int64, 1}(map(val -> round(val), XGBoost.predict(boost, x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJvc8asaAjyf",
        "outputId": "419e3b85-302e-4a9d-8ebf-f3f486a92235"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[22:54:41] WARNING: /workspace/srcdir/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[1]\ttrain-logloss:0.186739\n",
            "[2]\ttrain-logloss:0.090753\n",
            "[3]\ttrain-logloss:0.053734\n",
            "[4]\ttrain-logloss:0.035809\n",
            "[5]\ttrain-logloss:0.027452\n",
            "[6]\ttrain-logloss:0.021554\n",
            "[7]\ttrain-logloss:0.017867\n",
            "[8]\ttrain-logloss:0.015599\n",
            "[9]\ttrain-logloss:0.012860\n",
            "[10]\ttrain-logloss:0.011259\n",
            "[11]\ttrain-logloss:0.009873\n",
            "[12]\ttrain-logloss:0.009150\n",
            "[13]\ttrain-logloss:0.008803\n",
            "[14]\ttrain-logloss:0.008533\n",
            "[15]\ttrain-logloss:0.008328\n",
            "[16]\ttrain-logloss:0.008059\n",
            "[17]\ttrain-logloss:0.007796\n",
            "[18]\ttrain-logloss:0.007525\n",
            "[19]\ttrain-logloss:0.007355\n",
            "[20]\ttrain-logloss:0.007127\n",
            "[21]\ttrain-logloss:0.006959\n",
            "[22]\ttrain-logloss:0.006815\n",
            "[23]\ttrain-logloss:0.006647\n",
            "[24]\ttrain-logloss:0.006532\n",
            "[25]\ttrain-logloss:0.006415\n",
            "[26]\ttrain-logloss:0.006326\n",
            "[27]\ttrain-logloss:0.006224\n",
            "[28]\ttrain-logloss:0.006145\n",
            "[29]\ttrain-logloss:0.006067\n",
            "[30]\ttrain-logloss:0.005999\n",
            "[31]\ttrain-logloss:0.005933\n",
            "[32]\ttrain-logloss:0.005890\n",
            "[33]\ttrain-logloss:0.005825\n",
            "[34]\ttrain-logloss:0.005786\n",
            "[35]\ttrain-logloss:0.005737\n",
            "[36]\ttrain-logloss:0.005695\n",
            "[37]\ttrain-logloss:0.005667\n",
            "[38]\ttrain-logloss:0.005667\n",
            "[39]\ttrain-logloss:0.005667\n",
            "[40]\ttrain-logloss:0.005667\n",
            "[41]\ttrain-logloss:0.005667\n",
            "[42]\ttrain-logloss:0.005667\n",
            "[43]\ttrain-logloss:0.005667\n",
            "[44]\ttrain-logloss:0.005667\n",
            "[45]\ttrain-logloss:0.005667\n",
            "[46]\ttrain-logloss:0.005667\n",
            "[47]\ttrain-logloss:0.005667\n",
            "[48]\ttrain-logloss:0.005667\n",
            "[49]\ttrain-logloss:0.005667\n",
            "[50]\ttrain-logloss:0.005667\n",
            "[51]\ttrain-logloss:0.005667\n",
            "[52]\ttrain-logloss:0.005667\n",
            "[53]\ttrain-logloss:0.005667\n",
            "[54]\ttrain-logloss:0.005667\n",
            "[55]\ttrain-logloss:0.005667\n",
            "[56]\ttrain-logloss:0.005667\n",
            "[57]\ttrain-logloss:0.005667\n",
            "[58]\ttrain-logloss:0.005667\n",
            "[59]\ttrain-logloss:0.005667\n",
            "[60]\ttrain-logloss:0.005667\n",
            "[61]\ttrain-logloss:0.005667\n",
            "[62]\ttrain-logloss:0.005667\n",
            "[63]\ttrain-logloss:0.005667\n",
            "[64]\ttrain-logloss:0.005667\n",
            "[65]\ttrain-logloss:0.005667\n",
            "[66]\ttrain-logloss:0.005667\n",
            "[67]\ttrain-logloss:0.005667\n",
            "[68]\ttrain-logloss:0.005667\n",
            "[69]\ttrain-logloss:0.005667\n",
            "[70]\ttrain-logloss:0.005667\n",
            "[71]\ttrain-logloss:0.005667\n",
            "[72]\ttrain-logloss:0.005667\n",
            "[73]\ttrain-logloss:0.005667\n",
            "[74]\ttrain-logloss:0.005667\n",
            "[75]\ttrain-logloss:0.005667\n",
            "[76]\ttrain-logloss:0.005667\n",
            "[77]\ttrain-logloss:0.005667\n",
            "[78]\ttrain-logloss:0.005667\n",
            "[79]\ttrain-logloss:0.005667\n",
            "[80]\ttrain-logloss:0.005667\n",
            "[81]\ttrain-logloss:0.005667\n",
            "[82]\ttrain-logloss:0.005667\n",
            "[83]\ttrain-logloss:0.005667\n",
            "[84]\ttrain-logloss:0.005667\n",
            "[85]\ttrain-logloss:0.005667\n",
            "[86]\ttrain-logloss:0.005667\n",
            "[87]\ttrain-logloss:0.005667\n",
            "[88]\ttrain-logloss:0.005667\n",
            "[89]\ttrain-logloss:0.005667\n",
            "[90]\ttrain-logloss:0.005667\n",
            "[91]\ttrain-logloss:0.005667\n",
            "[92]\ttrain-logloss:0.005667\n",
            "[93]\ttrain-logloss:0.005667\n",
            "[94]\ttrain-logloss:0.005667\n",
            "[95]\ttrain-logloss:0.005667\n",
            "[96]\ttrain-logloss:0.005667\n",
            "[97]\ttrain-logloss:0.005667\n",
            "[98]\ttrain-logloss:0.005667\n",
            "[99]\ttrain-logloss:0.005667\n",
            "[100]\ttrain-logloss:0.005667\n",
            "[101]\ttrain-logloss:0.005667\n",
            "[102]\ttrain-logloss:0.005667\n",
            "[103]\ttrain-logloss:0.005667\n",
            "[104]\ttrain-logloss:0.005667\n",
            "[105]\ttrain-logloss:0.005667\n",
            "[106]\ttrain-logloss:0.005667\n",
            "[107]\ttrain-logloss:0.005667\n",
            "[108]\ttrain-logloss:0.005667\n",
            "[109]\ttrain-logloss:0.005667\n",
            "[110]\ttrain-logloss:0.005667\n",
            "[111]\ttrain-logloss:0.005667\n",
            "[112]\ttrain-logloss:0.005667\n",
            "[113]\ttrain-logloss:0.005667\n",
            "[114]\ttrain-logloss:0.005667\n",
            "[115]\ttrain-logloss:0.005667\n",
            "[116]\ttrain-logloss:0.005667\n",
            "[117]\ttrain-logloss:0.005667\n",
            "[118]\ttrain-logloss:0.005667\n",
            "[119]\ttrain-logloss:0.005667\n",
            "[120]\ttrain-logloss:0.005667\n",
            "[121]\ttrain-logloss:0.005667\n",
            "[122]\ttrain-logloss:0.005667\n",
            "[123]\ttrain-logloss:0.005667\n",
            "[124]\ttrain-logloss:0.005667\n",
            "[125]\ttrain-logloss:0.005667\n",
            "[126]\ttrain-logloss:0.005667\n",
            "[127]\ttrain-logloss:0.005667\n",
            "[128]\ttrain-logloss:0.005667\n",
            "[129]\ttrain-logloss:0.005667\n",
            "[130]\ttrain-logloss:0.005667\n",
            "[131]\ttrain-logloss:0.005667\n",
            "[132]\ttrain-logloss:0.005667\n",
            "[133]\ttrain-logloss:0.005667\n",
            "[134]\ttrain-logloss:0.005667\n",
            "[135]\ttrain-logloss:0.005667\n",
            "[136]\ttrain-logloss:0.005667\n",
            "[137]\ttrain-logloss:0.005667\n",
            "[138]\ttrain-logloss:0.005667\n",
            "[139]\ttrain-logloss:0.005667\n",
            "[140]\ttrain-logloss:0.005667\n",
            "[141]\ttrain-logloss:0.005667\n",
            "[142]\ttrain-logloss:0.005667\n",
            "[143]\ttrain-logloss:0.005667\n",
            "[144]\ttrain-logloss:0.005667\n",
            "[145]\ttrain-logloss:0.005667\n",
            "[146]\ttrain-logloss:0.005667\n",
            "[147]\ttrain-logloss:0.005667\n",
            "[148]\ttrain-logloss:0.005667\n",
            "[149]\ttrain-logloss:0.005667\n",
            "[150]\ttrain-logloss:0.005667\n",
            "[151]\ttrain-logloss:0.005667\n",
            "[152]\ttrain-logloss:0.005667\n",
            "[153]\ttrain-logloss:0.005667\n",
            "[154]\ttrain-logloss:0.005667\n",
            "[155]\ttrain-logloss:0.005667\n",
            "[156]\ttrain-logloss:0.005667\n",
            "[157]\ttrain-logloss:0.005667\n",
            "[158]\ttrain-logloss:0.005667\n",
            "[159]\ttrain-logloss:0.005667\n",
            "[160]\ttrain-logloss:0.005667\n",
            "[161]\ttrain-logloss:0.005667\n",
            "[162]\ttrain-logloss:0.005667\n",
            "[163]\ttrain-logloss:0.005667\n",
            "[164]\ttrain-logloss:0.005667\n",
            "[165]\ttrain-logloss:0.005667\n",
            "[166]\ttrain-logloss:0.005667\n",
            "[167]\ttrain-logloss:0.005667\n",
            "[168]\ttrain-logloss:0.005667\n",
            "[169]\ttrain-logloss:0.005667\n",
            "[170]\ttrain-logloss:0.005667\n",
            "[171]\ttrain-logloss:0.005667\n",
            "[172]\ttrain-logloss:0.005667\n",
            "[173]\ttrain-logloss:0.005667\n",
            "[174]\ttrain-logloss:0.005667\n",
            "[175]\ttrain-logloss:0.005667\n",
            "[176]\ttrain-logloss:0.005667\n",
            "[177]\ttrain-logloss:0.005667\n",
            "[178]\ttrain-logloss:0.005667\n",
            "[179]\ttrain-logloss:0.005667\n",
            "[180]\ttrain-logloss:0.005667\n",
            "[181]\ttrain-logloss:0.005667\n",
            "[182]\ttrain-logloss:0.005667\n",
            "[183]\ttrain-logloss:0.005667\n",
            "[184]\ttrain-logloss:0.005667\n",
            "[185]\ttrain-logloss:0.005667\n",
            "[186]\ttrain-logloss:0.005667\n",
            "[187]\ttrain-logloss:0.005667\n",
            "[188]\ttrain-logloss:0.005667\n",
            "[189]\ttrain-logloss:0.005667\n",
            "[190]\ttrain-logloss:0.005667\n",
            "[191]\ttrain-logloss:0.005667\n",
            "[192]\ttrain-logloss:0.005667\n",
            "[193]\ttrain-logloss:0.005667\n",
            "[194]\ttrain-logloss:0.005667\n",
            "[195]\ttrain-logloss:0.005667\n",
            "[196]\ttrain-logloss:0.005667\n",
            "[197]\ttrain-logloss:0.005667\n",
            "[198]\ttrain-logloss:0.005667\n",
            "[199]\ttrain-logloss:0.005667\n",
            "[200]\ttrain-logloss:0.005667\n",
            "[201]\ttrain-logloss:0.005667\n",
            "[202]\ttrain-logloss:0.005667\n",
            "[203]\ttrain-logloss:0.005667\n",
            "[204]\ttrain-logloss:0.005667\n",
            "[205]\ttrain-logloss:0.005667\n",
            "[206]\ttrain-logloss:0.005667\n",
            "[207]\ttrain-logloss:0.005667\n",
            "[208]\ttrain-logloss:0.005667\n",
            "[209]\ttrain-logloss:0.005667\n",
            "[210]\ttrain-logloss:0.005667\n",
            "[211]\ttrain-logloss:0.005667\n",
            "[212]\ttrain-logloss:0.005667\n",
            "[213]\ttrain-logloss:0.005667\n",
            "[214]\ttrain-logloss:0.005667\n",
            "[215]\ttrain-logloss:0.005667\n",
            "[216]\ttrain-logloss:0.005667\n",
            "[217]\ttrain-logloss:0.005667\n",
            "[218]\ttrain-logloss:0.005667\n",
            "[219]\ttrain-logloss:0.005667\n",
            "[220]\ttrain-logloss:0.005667\n",
            "[221]\ttrain-logloss:0.005667\n",
            "[222]\ttrain-logloss:0.005667\n",
            "[223]\ttrain-logloss:0.005667\n",
            "[224]\ttrain-logloss:0.005667\n",
            "[225]\ttrain-logloss:0.005667\n",
            "[226]\ttrain-logloss:0.005667\n",
            "[227]\ttrain-logloss:0.005667\n",
            "[228]\ttrain-logloss:0.005667\n",
            "[229]\ttrain-logloss:0.005667\n",
            "[230]\ttrain-logloss:0.005667\n",
            "[231]\ttrain-logloss:0.005667\n",
            "[232]\ttrain-logloss:0.005667\n",
            "[233]\ttrain-logloss:0.005667\n",
            "[234]\ttrain-logloss:0.005667\n",
            "[235]\ttrain-logloss:0.005667\n",
            "[236]\ttrain-logloss:0.005667\n",
            "[237]\ttrain-logloss:0.005667\n",
            "[238]\ttrain-logloss:0.005667\n",
            "[239]\ttrain-logloss:0.005667\n",
            "[240]\ttrain-logloss:0.005667\n",
            "[241]\ttrain-logloss:0.005667\n",
            "[242]\ttrain-logloss:0.005667\n",
            "[243]\ttrain-logloss:0.005667\n",
            "[244]\ttrain-logloss:0.005667\n",
            "[245]\ttrain-logloss:0.005667\n",
            "[246]\ttrain-logloss:0.005667\n",
            "[247]\ttrain-logloss:0.005667\n",
            "[248]\ttrain-logloss:0.005667\n",
            "[249]\ttrain-logloss:0.005667\n",
            "[250]\ttrain-logloss:0.005667\n",
            "[251]\ttrain-logloss:0.005667\n",
            "[252]\ttrain-logloss:0.005667\n",
            "[253]\ttrain-logloss:0.005667\n",
            "[254]\ttrain-logloss:0.005667\n",
            "[255]\ttrain-logloss:0.005667\n",
            "[256]\ttrain-logloss:0.005667\n",
            "[257]\ttrain-logloss:0.005667\n",
            "[258]\ttrain-logloss:0.005667\n",
            "[259]\ttrain-logloss:0.005667\n",
            "[260]\ttrain-logloss:0.005667\n",
            "[261]\ttrain-logloss:0.005667\n",
            "[262]\ttrain-logloss:0.005667\n",
            "[263]\ttrain-logloss:0.005667\n",
            "[264]\ttrain-logloss:0.005667\n",
            "[265]\ttrain-logloss:0.005667\n",
            "[266]\ttrain-logloss:0.005667\n",
            "[267]\ttrain-logloss:0.005667\n",
            "[268]\ttrain-logloss:0.005667\n",
            "[269]\ttrain-logloss:0.005667\n",
            "[270]\ttrain-logloss:0.005667\n",
            "[271]\ttrain-logloss:0.005667\n",
            "[272]\ttrain-logloss:0.005667\n",
            "[273]\ttrain-logloss:0.005667\n",
            "[274]\ttrain-logloss:0.005667\n",
            "[275]\ttrain-logloss:0.005667\n",
            "[276]\ttrain-logloss:0.005667\n",
            "[277]\ttrain-logloss:0.005667\n",
            "[278]\ttrain-logloss:0.005667\n",
            "[279]\ttrain-logloss:0.005667\n",
            "[280]\ttrain-logloss:0.005667\n",
            "[281]\ttrain-logloss:0.005667\n",
            "[282]\ttrain-logloss:0.005667\n",
            "[283]\ttrain-logloss:0.005667\n",
            "[284]\ttrain-logloss:0.005667\n",
            "[285]\ttrain-logloss:0.005667\n",
            "[286]\ttrain-logloss:0.005667\n",
            "[287]\ttrain-logloss:0.005667\n",
            "[288]\ttrain-logloss:0.005667\n",
            "[289]\ttrain-logloss:0.005667\n",
            "[290]\ttrain-logloss:0.005667\n",
            "[291]\ttrain-logloss:0.005667\n",
            "[292]\ttrain-logloss:0.005667\n",
            "[293]\ttrain-logloss:0.005667\n",
            "[294]\ttrain-logloss:0.005667\n",
            "[295]\ttrain-logloss:0.005667\n",
            "[296]\ttrain-logloss:0.005667\n",
            "[297]\ttrain-logloss:0.005667\n",
            "[298]\ttrain-logloss:0.005667\n",
            "[299]\ttrain-logloss:0.005667\n",
            "[300]\ttrain-logloss:0.005667\n",
            "[301]\ttrain-logloss:0.005667\n",
            "[302]\ttrain-logloss:0.005667\n",
            "[303]\ttrain-logloss:0.005667\n",
            "[304]\ttrain-logloss:0.005667\n",
            "[305]\ttrain-logloss:0.005667\n",
            "[306]\ttrain-logloss:0.005667\n",
            "[307]\ttrain-logloss:0.005667\n",
            "[308]\ttrain-logloss:0.005667\n",
            "[309]\ttrain-logloss:0.005667\n",
            "[310]\ttrain-logloss:0.005667\n",
            "[311]\ttrain-logloss:0.005667\n",
            "[312]\ttrain-logloss:0.005667\n",
            "[313]\ttrain-logloss:0.005667\n",
            "[314]\ttrain-logloss:0.005667\n",
            "[315]\ttrain-logloss:0.005667\n",
            "[316]\ttrain-logloss:0.005667\n",
            "[317]\ttrain-logloss:0.005667\n",
            "[318]\ttrain-logloss:0.005667\n",
            "[319]\ttrain-logloss:0.005667\n",
            "[320]\ttrain-logloss:0.005667\n",
            "[321]\ttrain-logloss:0.005667\n",
            "[322]\ttrain-logloss:0.005667\n",
            "[323]\ttrain-logloss:0.005667\n",
            "[324]\ttrain-logloss:0.005667\n",
            "[325]\ttrain-logloss:0.005667\n",
            "[326]\ttrain-logloss:0.005667\n",
            "[327]\ttrain-logloss:0.005667\n",
            "[328]\ttrain-logloss:0.005667\n",
            "[329]\ttrain-logloss:0.005667\n",
            "[330]\ttrain-logloss:0.005667\n",
            "[331]\ttrain-logloss:0.005667\n",
            "[332]\ttrain-logloss:0.005667\n",
            "[333]\ttrain-logloss:0.005667\n",
            "[334]\ttrain-logloss:0.005667\n",
            "[335]\ttrain-logloss:0.005667\n",
            "[336]\ttrain-logloss:0.005667\n",
            "[337]\ttrain-logloss:0.005667\n",
            "[338]\ttrain-logloss:0.005667\n",
            "[339]\ttrain-logloss:0.005667\n",
            "[340]\ttrain-logloss:0.005667\n",
            "[341]\ttrain-logloss:0.005667\n",
            "[342]\ttrain-logloss:0.005667\n",
            "[343]\ttrain-logloss:0.005667\n",
            "[344]\ttrain-logloss:0.005667\n",
            "[345]\ttrain-logloss:0.005667\n",
            "[346]\ttrain-logloss:0.005667\n",
            "[347]\ttrain-logloss:0.005667\n",
            "[348]\ttrain-logloss:0.005667\n",
            "[349]\ttrain-logloss:0.005667\n",
            "[350]\ttrain-logloss:0.005667\n",
            "[351]\ttrain-logloss:0.005667\n",
            "[352]\ttrain-logloss:0.005667\n",
            "[353]\ttrain-logloss:0.005667\n",
            "[354]\ttrain-logloss:0.005667\n",
            "[355]\ttrain-logloss:0.005667\n",
            "[356]\ttrain-logloss:0.005667\n",
            "[357]\ttrain-logloss:0.005667\n",
            "[358]\ttrain-logloss:0.005667\n",
            "[359]\ttrain-logloss:0.005667\n",
            "[360]\ttrain-logloss:0.005667\n",
            "[361]\ttrain-logloss:0.005667\n",
            "[362]\ttrain-logloss:0.005667\n",
            "[363]\ttrain-logloss:0.005667\n",
            "[364]\ttrain-logloss:0.005667\n",
            "[365]\ttrain-logloss:0.005667\n",
            "[366]\ttrain-logloss:0.005667\n",
            "[367]\ttrain-logloss:0.005667\n",
            "[368]\ttrain-logloss:0.005667\n",
            "[369]\ttrain-logloss:0.005667\n",
            "[370]\ttrain-logloss:0.005667\n",
            "[371]\ttrain-logloss:0.005667\n",
            "[372]\ttrain-logloss:0.005667\n",
            "[373]\ttrain-logloss:0.005667\n",
            "[374]\ttrain-logloss:0.005667\n",
            "[375]\ttrain-logloss:0.005667\n",
            "[376]\ttrain-logloss:0.005667\n",
            "[377]\ttrain-logloss:0.005667\n",
            "[378]\ttrain-logloss:0.005667\n",
            "[379]\ttrain-logloss:0.005667\n",
            "[380]\ttrain-logloss:0.005667\n",
            "[381]\ttrain-logloss:0.005667\n",
            "[382]\ttrain-logloss:0.005667\n",
            "[383]\ttrain-logloss:0.005667\n",
            "[384]\ttrain-logloss:0.005667\n",
            "[385]\ttrain-logloss:0.005667\n",
            "[386]\ttrain-logloss:0.005667\n",
            "[387]\ttrain-logloss:0.005667\n",
            "[388]\ttrain-logloss:0.005667\n",
            "[389]\ttrain-logloss:0.005667\n",
            "[390]\ttrain-logloss:0.005667\n",
            "[391]\ttrain-logloss:0.005667\n",
            "[392]\ttrain-logloss:0.005667\n",
            "[393]\ttrain-logloss:0.005667\n",
            "[394]\ttrain-logloss:0.005667\n",
            "[395]\ttrain-logloss:0.005667\n",
            "[396]\ttrain-logloss:0.005667\n",
            "[397]\ttrain-logloss:0.005667\n",
            "[398]\ttrain-logloss:0.005667\n",
            "[399]\ttrain-logloss:0.005667\n",
            "[400]\ttrain-logloss:0.005667\n",
            "[401]\ttrain-logloss:0.005667\n",
            "[402]\ttrain-logloss:0.005667\n",
            "[403]\ttrain-logloss:0.005667\n",
            "[404]\ttrain-logloss:0.005667\n",
            "[405]\ttrain-logloss:0.005667\n",
            "[406]\ttrain-logloss:0.005667\n",
            "[407]\ttrain-logloss:0.005667\n",
            "[408]\ttrain-logloss:0.005667\n",
            "[409]\ttrain-logloss:0.005667\n",
            "[410]\ttrain-logloss:0.005667\n",
            "[411]\ttrain-logloss:0.005667\n",
            "[412]\ttrain-logloss:0.005667\n",
            "[413]\ttrain-logloss:0.005667\n",
            "[414]\ttrain-logloss:0.005667\n",
            "[415]\ttrain-logloss:0.005667\n",
            "[416]\ttrain-logloss:0.005667\n",
            "[417]\ttrain-logloss:0.005667\n",
            "[418]\ttrain-logloss:0.005667\n",
            "[419]\ttrain-logloss:0.005667\n",
            "[420]\ttrain-logloss:0.005667\n",
            "[421]\ttrain-logloss:0.005667\n",
            "[422]\ttrain-logloss:0.005667\n",
            "[423]\ttrain-logloss:0.005667\n",
            "[424]\ttrain-logloss:0.005667\n",
            "[425]\ttrain-logloss:0.005667\n",
            "[426]\ttrain-logloss:0.005667\n",
            "[427]\ttrain-logloss:0.005667\n",
            "[428]\ttrain-logloss:0.005667\n",
            "[429]\ttrain-logloss:0.005667\n",
            "[430]\ttrain-logloss:0.005667\n",
            "[431]\ttrain-logloss:0.005667\n",
            "[432]\ttrain-logloss:0.005667\n",
            "[433]\ttrain-logloss:0.005667\n",
            "[434]\ttrain-logloss:0.005667\n",
            "[435]\ttrain-logloss:0.005667\n",
            "[436]\ttrain-logloss:0.005667\n",
            "[437]\ttrain-logloss:0.005667\n",
            "[438]\ttrain-logloss:0.005667\n",
            "[439]\ttrain-logloss:0.005667\n",
            "[440]\ttrain-logloss:0.005667\n",
            "[441]\ttrain-logloss:0.005667\n",
            "[442]\ttrain-logloss:0.005667\n",
            "[443]\ttrain-logloss:0.005667\n",
            "[444]\ttrain-logloss:0.005667\n",
            "[445]\ttrain-logloss:0.005667\n",
            "[446]\ttrain-logloss:0.005667\n",
            "[447]\ttrain-logloss:0.005667\n",
            "[448]\ttrain-logloss:0.005667\n",
            "[449]\ttrain-logloss:0.005667\n",
            "[450]\ttrain-logloss:0.005667\n",
            "[451]\ttrain-logloss:0.005667\n",
            "[452]\ttrain-logloss:0.005667\n",
            "[453]\ttrain-logloss:0.005667\n",
            "[454]\ttrain-logloss:0.005667\n",
            "[455]\ttrain-logloss:0.005667\n",
            "[456]\ttrain-logloss:0.005667\n",
            "[457]\ttrain-logloss:0.005667\n",
            "[458]\ttrain-logloss:0.005667\n",
            "[459]\ttrain-logloss:0.005667\n",
            "[460]\ttrain-logloss:0.005667\n",
            "[461]\ttrain-logloss:0.005667\n",
            "[462]\ttrain-logloss:0.005667\n",
            "[463]\ttrain-logloss:0.005667\n",
            "[464]\ttrain-logloss:0.005667\n",
            "[465]\ttrain-logloss:0.005667\n",
            "[466]\ttrain-logloss:0.005667\n",
            "[467]\ttrain-logloss:0.005667\n",
            "[468]\ttrain-logloss:0.005667\n",
            "[469]\ttrain-logloss:0.005667\n",
            "[470]\ttrain-logloss:0.005667\n",
            "[471]\ttrain-logloss:0.005667\n",
            "[472]\ttrain-logloss:0.005667\n",
            "[473]\ttrain-logloss:0.005667\n",
            "[474]\ttrain-logloss:0.005667\n",
            "[475]\ttrain-logloss:0.005667\n",
            "[476]\ttrain-logloss:0.005667\n",
            "[477]\ttrain-logloss:0.005667\n",
            "[478]\ttrain-logloss:0.005667\n",
            "[479]\ttrain-logloss:0.005667\n",
            "[480]\ttrain-logloss:0.005667\n",
            "[481]\ttrain-logloss:0.005667\n",
            "[482]\ttrain-logloss:0.005667\n",
            "[483]\ttrain-logloss:0.005667\n",
            "[484]\ttrain-logloss:0.005667\n",
            "[485]\ttrain-logloss:0.005667\n",
            "[486]\ttrain-logloss:0.005667\n",
            "[487]\ttrain-logloss:0.005667\n",
            "[488]\ttrain-logloss:0.005667\n",
            "[489]\ttrain-logloss:0.005667\n",
            "[490]\ttrain-logloss:0.005667\n",
            "[491]\ttrain-logloss:0.005667\n",
            "[492]\ttrain-logloss:0.005667\n",
            "[493]\ttrain-logloss:0.005667\n",
            "[494]\ttrain-logloss:0.005667\n",
            "[495]\ttrain-logloss:0.005667\n",
            "[496]\ttrain-logloss:0.005667\n",
            "[497]\ttrain-logloss:0.005667\n",
            "[498]\ttrain-logloss:0.005667\n",
            "[499]\ttrain-logloss:0.005667\n",
            "[500]\ttrain-logloss:0.005667\n",
            "[501]\ttrain-logloss:0.005667\n",
            "[502]\ttrain-logloss:0.005667\n",
            "[503]\ttrain-logloss:0.005667\n",
            "[504]\ttrain-logloss:0.005667\n",
            "[505]\ttrain-logloss:0.005667\n",
            "[506]\ttrain-logloss:0.005667\n",
            "[507]\ttrain-logloss:0.005667\n",
            "[508]\ttrain-logloss:0.005667\n",
            "[509]\ttrain-logloss:0.005667\n",
            "[510]\ttrain-logloss:0.005667\n",
            "[511]\ttrain-logloss:0.005667\n",
            "[512]\ttrain-logloss:0.005667\n",
            "[513]\ttrain-logloss:0.005667\n",
            "[514]\ttrain-logloss:0.005667\n",
            "[515]\ttrain-logloss:0.005667\n",
            "[516]\ttrain-logloss:0.005667\n",
            "[517]\ttrain-logloss:0.005667\n",
            "[518]\ttrain-logloss:0.005667\n",
            "[519]\ttrain-logloss:0.005667\n",
            "[520]\ttrain-logloss:0.005667\n",
            "[521]\ttrain-logloss:0.005667\n",
            "[522]\ttrain-logloss:0.005667\n",
            "[523]\ttrain-logloss:0.005667\n",
            "[524]\ttrain-logloss:0.005667\n",
            "[525]\ttrain-logloss:0.005667\n",
            "[526]\ttrain-logloss:0.005667\n",
            "[527]\ttrain-logloss:0.005667\n",
            "[528]\ttrain-logloss:0.005667\n",
            "[529]\ttrain-logloss:0.005667\n",
            "[530]\ttrain-logloss:0.005667\n",
            "[531]\ttrain-logloss:0.005667\n",
            "[532]\ttrain-logloss:0.005667\n",
            "[533]\ttrain-logloss:0.005667\n",
            "[534]\ttrain-logloss:0.005667\n",
            "[535]\ttrain-logloss:0.005667\n",
            "[536]\ttrain-logloss:0.005667\n",
            "[537]\ttrain-logloss:0.005667\n",
            "[538]\ttrain-logloss:0.005667\n",
            "[539]\ttrain-logloss:0.005667\n",
            "[540]\ttrain-logloss:0.005667\n",
            "[541]\ttrain-logloss:0.005667\n",
            "[542]\ttrain-logloss:0.005667\n",
            "[543]\ttrain-logloss:0.005667\n",
            "[544]\ttrain-logloss:0.005667\n",
            "[545]\ttrain-logloss:0.005667\n",
            "[546]\ttrain-logloss:0.005667\n",
            "[547]\ttrain-logloss:0.005667\n",
            "[548]\ttrain-logloss:0.005667\n",
            "[549]\ttrain-logloss:0.005667\n",
            "[550]\ttrain-logloss:0.005667\n",
            "[551]\ttrain-logloss:0.005667\n",
            "[552]\ttrain-logloss:0.005667\n",
            "[553]\ttrain-logloss:0.005667\n",
            "[554]\ttrain-logloss:0.005667\n",
            "[555]\ttrain-logloss:0.005667\n",
            "[556]\ttrain-logloss:0.005667\n",
            "[557]\ttrain-logloss:0.005667\n",
            "[558]\ttrain-logloss:0.005667\n",
            "[559]\ttrain-logloss:0.005667\n",
            "[560]\ttrain-logloss:0.005667\n",
            "[561]\ttrain-logloss:0.005667\n",
            "[562]\ttrain-logloss:0.005667\n",
            "[563]\ttrain-logloss:0.005667\n",
            "[564]\ttrain-logloss:0.005667\n",
            "[565]\ttrain-logloss:0.005667\n",
            "[566]\ttrain-logloss:0.005667\n",
            "[567]\ttrain-logloss:0.005667\n",
            "[568]\ttrain-logloss:0.005667\n",
            "[569]\ttrain-logloss:0.005667\n",
            "[570]\ttrain-logloss:0.005667\n",
            "[571]\ttrain-logloss:0.005667\n",
            "[572]\ttrain-logloss:0.005667\n",
            "[573]\ttrain-logloss:0.005667\n",
            "[574]\ttrain-logloss:0.005667\n",
            "[575]\ttrain-logloss:0.005667\n",
            "[576]\ttrain-logloss:0.005667\n",
            "[577]\ttrain-logloss:0.005667\n",
            "[578]\ttrain-logloss:0.005667\n",
            "[579]\ttrain-logloss:0.005667\n",
            "[580]\ttrain-logloss:0.005667\n",
            "[581]\ttrain-logloss:0.005667\n",
            "[582]\ttrain-logloss:0.005667\n",
            "[583]\ttrain-logloss:0.005667\n",
            "[584]\ttrain-logloss:0.005667\n",
            "[585]\ttrain-logloss:0.005667\n",
            "[586]\ttrain-logloss:0.005667\n",
            "[587]\ttrain-logloss:0.005667\n",
            "[588]\ttrain-logloss:0.005667\n",
            "[589]\ttrain-logloss:0.005667\n",
            "[590]\ttrain-logloss:0.005667\n",
            "[591]\ttrain-logloss:0.005667\n",
            "[592]\ttrain-logloss:0.005667\n",
            "[593]\ttrain-logloss:0.005667\n",
            "[594]\ttrain-logloss:0.005667\n",
            "[595]\ttrain-logloss:0.005667\n",
            "[596]\ttrain-logloss:0.005667\n",
            "[597]\ttrain-logloss:0.005667\n",
            "[598]\ttrain-logloss:0.005667\n",
            "[599]\ttrain-logloss:0.005667\n",
            "[600]\ttrain-logloss:0.005667\n",
            "[601]\ttrain-logloss:0.005667\n",
            "[602]\ttrain-logloss:0.005667\n",
            "[603]\ttrain-logloss:0.005667\n",
            "[604]\ttrain-logloss:0.005667\n",
            "[605]\ttrain-logloss:0.005667\n",
            "[606]\ttrain-logloss:0.005667\n",
            "[607]\ttrain-logloss:0.005667\n",
            "[608]\ttrain-logloss:0.005667\n",
            "[609]\ttrain-logloss:0.005667\n",
            "[610]\ttrain-logloss:0.005667\n",
            "[611]\ttrain-logloss:0.005667\n",
            "[612]\ttrain-logloss:0.005667\n",
            "[613]\ttrain-logloss:0.005667\n",
            "[614]\ttrain-logloss:0.005667\n",
            "[615]\ttrain-logloss:0.005667\n",
            "[616]\ttrain-logloss:0.005667\n",
            "[617]\ttrain-logloss:0.005667\n",
            "[618]\ttrain-logloss:0.005667\n",
            "[619]\ttrain-logloss:0.005667\n",
            "[620]\ttrain-logloss:0.005667\n",
            "[621]\ttrain-logloss:0.005667\n",
            "[622]\ttrain-logloss:0.005667\n",
            "[623]\ttrain-logloss:0.005667\n",
            "[624]\ttrain-logloss:0.005667\n",
            "[625]\ttrain-logloss:0.005667\n",
            "[626]\ttrain-logloss:0.005667\n",
            "[627]\ttrain-logloss:0.005667\n",
            "[628]\ttrain-logloss:0.005667\n",
            "[629]\ttrain-logloss:0.005667\n",
            "[630]\ttrain-logloss:0.005667\n",
            "[631]\ttrain-logloss:0.005667\n",
            "[632]\ttrain-logloss:0.005667\n",
            "[633]\ttrain-logloss:0.005667\n",
            "[634]\ttrain-logloss:0.005667\n",
            "[635]\ttrain-logloss:0.005667\n",
            "[636]\ttrain-logloss:0.005667\n",
            "[637]\ttrain-logloss:0.005667\n",
            "[638]\ttrain-logloss:0.005667\n",
            "[639]\ttrain-logloss:0.005667\n",
            "[640]\ttrain-logloss:0.005667\n",
            "[641]\ttrain-logloss:0.005667\n",
            "[642]\ttrain-logloss:0.005667\n",
            "[643]\ttrain-logloss:0.005667\n",
            "[644]\ttrain-logloss:0.005667\n",
            "[645]\ttrain-logloss:0.005667\n",
            "[646]\ttrain-logloss:0.005667\n",
            "[647]\ttrain-logloss:0.005667\n",
            "[648]\ttrain-logloss:0.005667\n",
            "[649]\ttrain-logloss:0.005667\n",
            "[650]\ttrain-logloss:0.005667\n",
            "[651]\ttrain-logloss:0.005667\n",
            "[652]\ttrain-logloss:0.005667\n",
            "[653]\ttrain-logloss:0.005667\n",
            "[654]\ttrain-logloss:0.005667\n",
            "[655]\ttrain-logloss:0.005667\n",
            "[656]\ttrain-logloss:0.005667\n",
            "[657]\ttrain-logloss:0.005667\n",
            "[658]\ttrain-logloss:0.005667\n",
            "[659]\ttrain-logloss:0.005667\n",
            "[660]\ttrain-logloss:0.005667\n",
            "[661]\ttrain-logloss:0.005667\n",
            "[662]\ttrain-logloss:0.005667\n",
            "[663]\ttrain-logloss:0.005667\n",
            "[664]\ttrain-logloss:0.005667\n",
            "[665]\ttrain-logloss:0.005667\n",
            "[666]\ttrain-logloss:0.005667\n",
            "[667]\ttrain-logloss:0.005667\n",
            "[668]\ttrain-logloss:0.005667\n",
            "[669]\ttrain-logloss:0.005667\n",
            "[670]\ttrain-logloss:0.005667\n",
            "[671]\ttrain-logloss:0.005667\n",
            "[672]\ttrain-logloss:0.005667\n",
            "[673]\ttrain-logloss:0.005667\n",
            "[674]\ttrain-logloss:0.005667\n",
            "[675]\ttrain-logloss:0.005667\n",
            "[676]\ttrain-logloss:0.005667\n",
            "[677]\ttrain-logloss:0.005667\n",
            "[678]\ttrain-logloss:0.005667\n",
            "[679]\ttrain-logloss:0.005667\n",
            "[680]\ttrain-logloss:0.005667\n",
            "[681]\ttrain-logloss:0.005667\n",
            "[682]\ttrain-logloss:0.005667\n",
            "[683]\ttrain-logloss:0.005667\n",
            "[684]\ttrain-logloss:0.005667\n",
            "[685]\ttrain-logloss:0.005667\n",
            "[686]\ttrain-logloss:0.005667\n",
            "[687]\ttrain-logloss:0.005667\n",
            "[688]\ttrain-logloss:0.005667\n",
            "[689]\ttrain-logloss:0.005667\n",
            "[690]\ttrain-logloss:0.005667\n",
            "[691]\ttrain-logloss:0.005667\n",
            "[692]\ttrain-logloss:0.005667\n",
            "[693]\ttrain-logloss:0.005667\n",
            "[694]\ttrain-logloss:0.005667\n",
            "[695]\ttrain-logloss:0.005667\n",
            "[696]\ttrain-logloss:0.005667\n",
            "[697]\ttrain-logloss:0.005667\n",
            "[698]\ttrain-logloss:0.005667\n",
            "[699]\ttrain-logloss:0.005667\n",
            "[700]\ttrain-logloss:0.005667\n",
            "[701]\ttrain-logloss:0.005667\n",
            "[702]\ttrain-logloss:0.005667\n",
            "[703]\ttrain-logloss:0.005667\n",
            "[704]\ttrain-logloss:0.005667\n",
            "[705]\ttrain-logloss:0.005667\n",
            "[706]\ttrain-logloss:0.005667\n",
            "[707]\ttrain-logloss:0.005667\n",
            "[708]\ttrain-logloss:0.005667\n",
            "[709]\ttrain-logloss:0.005667\n",
            "[710]\ttrain-logloss:0.005667\n",
            "[711]\ttrain-logloss:0.005667\n",
            "[712]\ttrain-logloss:0.005667\n",
            "[713]\ttrain-logloss:0.005667\n",
            "[714]\ttrain-logloss:0.005667\n",
            "[715]\ttrain-logloss:0.005667\n",
            "[716]\ttrain-logloss:0.005667\n",
            "[717]\ttrain-logloss:0.005667\n",
            "[718]\ttrain-logloss:0.005667\n",
            "[719]\ttrain-logloss:0.005667\n",
            "[720]\ttrain-logloss:0.005667\n",
            "[721]\ttrain-logloss:0.005667\n",
            "[722]\ttrain-logloss:0.005667\n",
            "[723]\ttrain-logloss:0.005667\n",
            "[724]\ttrain-logloss:0.005667\n",
            "[725]\ttrain-logloss:0.005667\n",
            "[726]\ttrain-logloss:0.005667\n",
            "[727]\ttrain-logloss:0.005667\n",
            "[728]\ttrain-logloss:0.005667\n",
            "[729]\ttrain-logloss:0.005667\n",
            "[730]\ttrain-logloss:0.005667\n",
            "[731]\ttrain-logloss:0.005667\n",
            "[732]\ttrain-logloss:0.005667\n",
            "[733]\ttrain-logloss:0.005667\n",
            "[734]\ttrain-logloss:0.005667\n",
            "[735]\ttrain-logloss:0.005667\n",
            "[736]\ttrain-logloss:0.005667\n",
            "[737]\ttrain-logloss:0.005667\n",
            "[738]\ttrain-logloss:0.005667\n",
            "[739]\ttrain-logloss:0.005667\n",
            "[740]\ttrain-logloss:0.005667\n",
            "[741]\ttrain-logloss:0.005667\n",
            "[742]\ttrain-logloss:0.005667\n",
            "[743]\ttrain-logloss:0.005667\n",
            "[744]\ttrain-logloss:0.005667\n",
            "[745]\ttrain-logloss:0.005667\n",
            "[746]\ttrain-logloss:0.005667\n",
            "[747]\ttrain-logloss:0.005667\n",
            "[748]\ttrain-logloss:0.005667\n",
            "[749]\ttrain-logloss:0.005667\n",
            "[750]\ttrain-logloss:0.005667\n",
            "[751]\ttrain-logloss:0.005667\n",
            "[752]\ttrain-logloss:0.005667\n",
            "[753]\ttrain-logloss:0.005667\n",
            "[754]\ttrain-logloss:0.005667\n",
            "[755]\ttrain-logloss:0.005667\n",
            "[756]\ttrain-logloss:0.005667\n",
            "[757]\ttrain-logloss:0.005667\n",
            "[758]\ttrain-logloss:0.005667\n",
            "[759]\ttrain-logloss:0.005667\n",
            "[760]\ttrain-logloss:0.005667\n",
            "[761]\ttrain-logloss:0.005667\n",
            "[762]\ttrain-logloss:0.005667\n",
            "[763]\ttrain-logloss:0.005667\n",
            "[764]\ttrain-logloss:0.005667\n",
            "[765]\ttrain-logloss:0.005667\n",
            "[766]\ttrain-logloss:0.005667\n",
            "[767]\ttrain-logloss:0.005667\n",
            "[768]\ttrain-logloss:0.005667\n",
            "[769]\ttrain-logloss:0.005667\n",
            "[770]\ttrain-logloss:0.005667\n",
            "[771]\ttrain-logloss:0.005667\n",
            "[772]\ttrain-logloss:0.005667\n",
            "[773]\ttrain-logloss:0.005667\n",
            "[774]\ttrain-logloss:0.005667\n",
            "[775]\ttrain-logloss:0.005667\n",
            "[776]\ttrain-logloss:0.005667\n",
            "[777]\ttrain-logloss:0.005667\n",
            "[778]\ttrain-logloss:0.005667\n",
            "[779]\ttrain-logloss:0.005667\n",
            "[780]\ttrain-logloss:0.005667\n",
            "[781]\ttrain-logloss:0.005667\n",
            "[782]\ttrain-logloss:0.005667\n",
            "[783]\ttrain-logloss:0.005667\n",
            "[784]\ttrain-logloss:0.005667\n",
            "[785]\ttrain-logloss:0.005667\n",
            "[786]\ttrain-logloss:0.005667\n",
            "[787]\ttrain-logloss:0.005667\n",
            "[788]\ttrain-logloss:0.005667\n",
            "[789]\ttrain-logloss:0.005667\n",
            "[790]\ttrain-logloss:0.005667\n",
            "[791]\ttrain-logloss:0.005667\n",
            "[792]\ttrain-logloss:0.005667\n",
            "[793]\ttrain-logloss:0.005667\n",
            "[794]\ttrain-logloss:0.005667\n",
            "[795]\ttrain-logloss:0.005667\n",
            "[796]\ttrain-logloss:0.005667\n",
            "[797]\ttrain-logloss:0.005667\n",
            "[798]\ttrain-logloss:0.005667\n",
            "[799]\ttrain-logloss:0.005667\n",
            "[800]\ttrain-logloss:0.005667\n",
            "[801]\ttrain-logloss:0.005667\n",
            "[802]\ttrain-logloss:0.005667\n",
            "[803]\ttrain-logloss:0.005667\n",
            "[804]\ttrain-logloss:0.005667\n",
            "[805]\ttrain-logloss:0.005667\n",
            "[806]\ttrain-logloss:0.005667\n",
            "[807]\ttrain-logloss:0.005667\n",
            "[808]\ttrain-logloss:0.005667\n",
            "[809]\ttrain-logloss:0.005667\n",
            "[810]\ttrain-logloss:0.005667\n",
            "[811]\ttrain-logloss:0.005667\n",
            "[812]\ttrain-logloss:0.005667\n",
            "[813]\ttrain-logloss:0.005667\n",
            "[814]\ttrain-logloss:0.005667\n",
            "[815]\ttrain-logloss:0.005667\n",
            "[816]\ttrain-logloss:0.005667\n",
            "[817]\ttrain-logloss:0.005667\n",
            "[818]\ttrain-logloss:0.005667\n",
            "[819]\ttrain-logloss:0.005667\n",
            "[820]\ttrain-logloss:0.005667\n",
            "[821]\ttrain-logloss:0.005667\n",
            "[822]\ttrain-logloss:0.005667\n",
            "[823]\ttrain-logloss:0.005667\n",
            "[824]\ttrain-logloss:0.005667\n",
            "[825]\ttrain-logloss:0.005667\n",
            "[826]\ttrain-logloss:0.005667\n",
            "[827]\ttrain-logloss:0.005667\n",
            "[828]\ttrain-logloss:0.005667\n",
            "[829]\ttrain-logloss:0.005667\n",
            "[830]\ttrain-logloss:0.005667\n",
            "[831]\ttrain-logloss:0.005667\n",
            "[832]\ttrain-logloss:0.005667\n",
            "[833]\ttrain-logloss:0.005667\n",
            "[834]\ttrain-logloss:0.005667\n",
            "[835]\ttrain-logloss:0.005667\n",
            "[836]\ttrain-logloss:0.005667\n",
            "[837]\ttrain-logloss:0.005667\n",
            "[838]\ttrain-logloss:0.005667\n",
            "[839]\ttrain-logloss:0.005667\n",
            "[840]\ttrain-logloss:0.005667\n",
            "[841]\ttrain-logloss:0.005667\n",
            "[842]\ttrain-logloss:0.005667\n",
            "[843]\ttrain-logloss:0.005667\n",
            "[844]\ttrain-logloss:0.005667\n",
            "[845]\ttrain-logloss:0.005667\n",
            "[846]\ttrain-logloss:0.005667\n",
            "[847]\ttrain-logloss:0.005667\n",
            "[848]\ttrain-logloss:0.005667\n",
            "[849]\ttrain-logloss:0.005667\n",
            "[850]\ttrain-logloss:0.005667\n",
            "[851]\ttrain-logloss:0.005667\n",
            "[852]\ttrain-logloss:0.005667\n",
            "[853]\ttrain-logloss:0.005667\n",
            "[854]\ttrain-logloss:0.005667\n",
            "[855]\ttrain-logloss:0.005667\n",
            "[856]\ttrain-logloss:0.005667\n",
            "[857]\ttrain-logloss:0.005667\n",
            "[858]\ttrain-logloss:0.005667\n",
            "[859]\ttrain-logloss:0.005667\n",
            "[860]\ttrain-logloss:0.005667\n",
            "[861]\ttrain-logloss:0.005667\n",
            "[862]\ttrain-logloss:0.005667\n",
            "[863]\ttrain-logloss:0.005667\n",
            "[864]\ttrain-logloss:0.005667\n",
            "[865]\ttrain-logloss:0.005667\n",
            "[866]\ttrain-logloss:0.005667\n",
            "[867]\ttrain-logloss:0.005667\n",
            "[868]\ttrain-logloss:0.005667\n",
            "[869]\ttrain-logloss:0.005667\n",
            "[870]\ttrain-logloss:0.005667\n",
            "[871]\ttrain-logloss:0.005667\n",
            "[872]\ttrain-logloss:0.005667\n",
            "[873]\ttrain-logloss:0.005667\n",
            "[874]\ttrain-logloss:0.005667\n",
            "[875]\ttrain-logloss:0.005667\n",
            "[876]\ttrain-logloss:0.005667\n",
            "[877]\ttrain-logloss:0.005667\n",
            "[878]\ttrain-logloss:0.005667\n",
            "[879]\ttrain-logloss:0.005667\n",
            "[880]\ttrain-logloss:0.005667\n",
            "[881]\ttrain-logloss:0.005667\n",
            "[882]\ttrain-logloss:0.005667\n",
            "[883]\ttrain-logloss:0.005667\n",
            "[884]\ttrain-logloss:0.005667\n",
            "[885]\ttrain-logloss:0.005667\n",
            "[886]\ttrain-logloss:0.005667\n",
            "[887]\ttrain-logloss:0.005667\n",
            "[888]\ttrain-logloss:0.005667\n",
            "[889]\ttrain-logloss:0.005667\n",
            "[890]\ttrain-logloss:0.005667\n",
            "[891]\ttrain-logloss:0.005667\n",
            "[892]\ttrain-logloss:0.005667\n",
            "[893]\ttrain-logloss:0.005667\n",
            "[894]\ttrain-logloss:0.005667\n",
            "[895]\ttrain-logloss:0.005667\n",
            "[896]\ttrain-logloss:0.005667\n",
            "[897]\ttrain-logloss:0.005667\n",
            "[898]\ttrain-logloss:0.005667\n",
            "[899]\ttrain-logloss:0.005667\n",
            "[900]\ttrain-logloss:0.005667\n",
            "[901]\ttrain-logloss:0.005667\n",
            "[902]\ttrain-logloss:0.005667\n",
            "[903]\ttrain-logloss:0.005667\n",
            "[904]\ttrain-logloss:0.005667\n",
            "[905]\ttrain-logloss:0.005667\n",
            "[906]\ttrain-logloss:0.005667\n",
            "[907]\ttrain-logloss:0.005667\n",
            "[908]\ttrain-logloss:0.005667\n",
            "[909]\ttrain-logloss:0.005667\n",
            "[910]\ttrain-logloss:0.005667\n",
            "[911]\ttrain-logloss:0.005667\n",
            "[912]\ttrain-logloss:0.005667\n",
            "[913]\ttrain-logloss:0.005667\n",
            "[914]\ttrain-logloss:0.005667\n",
            "[915]\ttrain-logloss:0.005667\n",
            "[916]\ttrain-logloss:0.005667\n",
            "[917]\ttrain-logloss:0.005667\n",
            "[918]\ttrain-logloss:0.005667\n",
            "[919]\ttrain-logloss:0.005667\n",
            "[920]\ttrain-logloss:0.005667\n",
            "[921]\ttrain-logloss:0.005667\n",
            "[922]\ttrain-logloss:0.005667\n",
            "[923]\ttrain-logloss:0.005667\n",
            "[924]\ttrain-logloss:0.005667\n",
            "[925]\ttrain-logloss:0.005667\n",
            "[926]\ttrain-logloss:0.005667\n",
            "[927]\ttrain-logloss:0.005667\n",
            "[928]\ttrain-logloss:0.005667\n",
            "[929]\ttrain-logloss:0.005667\n",
            "[930]\ttrain-logloss:0.005667\n",
            "[931]\ttrain-logloss:0.005667\n",
            "[932]\ttrain-logloss:0.005667\n",
            "[933]\ttrain-logloss:0.005667\n",
            "[934]\ttrain-logloss:0.005667\n",
            "[935]\ttrain-logloss:0.005667\n",
            "[936]\ttrain-logloss:0.005667\n",
            "[937]\ttrain-logloss:0.005667\n",
            "[938]\ttrain-logloss:0.005667\n",
            "[939]\ttrain-logloss:0.005667\n",
            "[940]\ttrain-logloss:0.005667\n",
            "[941]\ttrain-logloss:0.005667\n",
            "[942]\ttrain-logloss:0.005667\n",
            "[943]\ttrain-logloss:0.005667\n",
            "[944]\ttrain-logloss:0.005667\n",
            "[945]\ttrain-logloss:0.005667\n",
            "[946]\ttrain-logloss:0.005667\n",
            "[947]\ttrain-logloss:0.005667\n",
            "[948]\ttrain-logloss:0.005667\n",
            "[949]\ttrain-logloss:0.005667\n",
            "[950]\ttrain-logloss:0.005667\n",
            "[951]\ttrain-logloss:0.005667\n",
            "[952]\ttrain-logloss:0.005667\n",
            "[953]\ttrain-logloss:0.005667\n",
            "[954]\ttrain-logloss:0.005667\n",
            "[955]\ttrain-logloss:0.005667\n",
            "[956]\ttrain-logloss:0.005667\n",
            "[957]\ttrain-logloss:0.005667\n",
            "[958]\ttrain-logloss:0.005667\n",
            "[959]\ttrain-logloss:0.005667\n",
            "[960]\ttrain-logloss:0.005667\n",
            "[961]\ttrain-logloss:0.005667\n",
            "[962]\ttrain-logloss:0.005667\n",
            "[963]\ttrain-logloss:0.005667\n",
            "[964]\ttrain-logloss:0.005667\n",
            "[965]\ttrain-logloss:0.005667\n",
            "[966]\ttrain-logloss:0.005667\n",
            "[967]\ttrain-logloss:0.005667\n",
            "[968]\ttrain-logloss:0.005667\n",
            "[969]\ttrain-logloss:0.005667\n",
            "[970]\ttrain-logloss:0.005667\n",
            "[971]\ttrain-logloss:0.005667\n",
            "[972]\ttrain-logloss:0.005667\n",
            "[973]\ttrain-logloss:0.005667\n",
            "[974]\ttrain-logloss:0.005667\n",
            "[975]\ttrain-logloss:0.005667\n",
            "[976]\ttrain-logloss:0.005667\n",
            "[977]\ttrain-logloss:0.005667\n",
            "[978]\ttrain-logloss:0.005667\n",
            "[979]\ttrain-logloss:0.005667\n",
            "[980]\ttrain-logloss:0.005667\n",
            "[981]\ttrain-logloss:0.005667\n",
            "[982]\ttrain-logloss:0.005667\n",
            "[983]\ttrain-logloss:0.005667\n",
            "[984]\ttrain-logloss:0.005667\n",
            "[985]\ttrain-logloss:0.005667\n",
            "[986]\ttrain-logloss:0.005667\n",
            "[987]\ttrain-logloss:0.005667\n",
            "[988]\ttrain-logloss:0.005667\n",
            "[989]\ttrain-logloss:0.005667\n",
            "[990]\ttrain-logloss:0.005667\n",
            "[991]\ttrain-logloss:0.005667\n",
            "[992]\ttrain-logloss:0.005667\n",
            "[993]\ttrain-logloss:0.005667\n",
            "[994]\ttrain-logloss:0.005667\n",
            "[995]\ttrain-logloss:0.005667\n",
            "[996]\ttrain-logloss:0.005667\n",
            "[997]\ttrain-logloss:0.005667\n",
            "[998]\ttrain-logloss:0.005667\n",
            "[999]\ttrain-logloss:0.005667\n",
            "[1000]\ttrain-logloss:0.005667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "187-element Vector{Int64}:\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 1\n",
              " 0\n",
              " 0\n",
              " 1\n",
              " ⋮\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 1\n",
              " 0\n",
              " 0\n",
              " 0\n",
              " 0"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "errorrate(y_test, prediction_rounded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ2tOfdGOOBS",
        "outputId": "fff2cc11-530d-47c1-9e56-92b485254287"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.026737967914438502"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLBase.confusmat(2, Array{Int64, 1}(y_test .+1), Array{Int64,1}(prediction_rounded .+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jucXhEaaOSmB",
        "outputId": "66f82d2f-499e-4bff-de07-7f6f60d107ee"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×2 Matrix{Int64}:\n",
              " 120   4\n",
              "   1  62"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Vary hyperparameters**\n",
        "\n",
        "Run 1\n",
        "\n",
        "dtrain, num_round = 2, 100, eta = 1 , objective = \"binary:logistic\"\n",
        "\n",
        "0.026737967914438502\n",
        "\n",
        "2×2 Matrix{Int64}:\n",
        "\n",
        " 120   4\n",
        "\n",
        "   1  62\n",
        "\n",
        "Run 2\n",
        "\n"
      ],
      "metadata": {
        "id": "jR8I_XoW-2tb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UMidUQB03vJ"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/JuliaLang/julia-logo-graphics/master/images/julia-logo-mask.png\" height=\"100\" />"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Julia_Colab_Notebook_Template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}